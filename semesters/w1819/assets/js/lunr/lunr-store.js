var store = [{
        "title": "Lecture 1: Multidimensional signals and systems",
        "excerpt":"Notation In our course, we will deal almost exclusively with real functions. A scalar function will be denoted as $f : \\RR^d \\rightarrow \\RR$, $f(\\bb{x})$,or simply $f$, with $d$ denoting the domain dimension. $d$-dimensional multi-indices will be denoted by $\\bb{n} = (n_1,\\dots,n_d) \\in \\bb{Z}^d$.An operator acting on a function will be denoted by $\\mathcal{H}(f)$ or simply $\\mathcal{H}f$. Multidimensional signals and systems A $d$-dimensional signal is a real- (or sometimes complex-) valued function $f : \\RR^d \\rightarrow \\RR$. In the case of $d=2$, we will refer to such signals as ‘‘images’’ and interpret the argument of the function, $\\bb{x} = (x_1,x_2)$ as the spatial location of a point in an image; for $d=3$, we will refer to such signals as to images, interpreting the additional third dimension as ‘‘time’’. In the one-dimensional case,  the scalar argument will be denoted by $t$ and interpreted as ‘‘time’’. Denoting the space of real-valued signals as $\\mathbb{F}(\\RR^d,\\RR) = {f  :  f : \\RR^d \\rightarrow \\RR }$, a system is an operator $\\mathcal{H} : \\mathbb{F}(\\RR^d,\\RR) \\rightarrow \\mathbb{F}(\\RR^d,\\RR)$. That is, a system $\\mathcal{H}$ accepts a signal  as the input and produces another signal as the output.For example the system receiving $f(\\bb{x})$ and producing $f(\\bb{x}-\\bb{p})$ will be called translation (or shift) and the corresponding operator will be denoted as $\\tau_{\\bb{p}}$. With this notation we can write $(\\tau_{\\bb{p}} f)(\\bb{x}) = f(\\bb{x}-\\bb{p})$. Linearity A system $\\mathcal{H}$ is said to be a linear system is it is additive and homogenous, i.e., for every $f,g \\in \\mathbb{F}(\\RR^d,\\RR)$ and $a, b \\in \\RR$, The output of (almost) every linear system can be represented as a linear functional of the form where the function (more generally, distribution – see in the sequel)  $h(\\bb{x},\\bb{x}’)$ is called the impulse response of the system. Informally, $h(\\bb{x},\\bb{x}’)$ tells what will be the output of the system at point $\\bb{x}$ when the input is an impulse at $\\bb{x}’$.An alternative way to view the above description is by defining the standard inner product on $L^2(\\RR) \\subset \\mathbb{F}(\\RR^d,\\RR)$: since most of the time we will be interested in real-valued functions, we will often omit the complex conjugate from the second argument. Shift invariance A linear system $\\mathcal{H}$ is called shift invariant (LSI for short) if it commutes with the translation operator, i.e., for every $\\bb{p} \\in \\RR^d$ In other words, the output of the system given a shifted input is the same as the output of the system shifted by the same amount. Substituting a specific input $f$ and shift $\\bb{p}$, we have the identity Since the latter holds for every $h$ and $\\bb{p}$, we have $h(\\bb{x}-\\bb{p}, \\bb{x}’) = h(\\bb{x}, \\bb{x}’+\\bb{p})$ at every $\\bb{x}$. In other words, $h(\\bb{x},\\bb{x}’)$ is effectively only a function of $\\bb{x}-\\bb{x}’$, which we will continue denoting as ‘‘h’’ with some abuse of notation. This particular structure is called Toeplitz and is the multidimensional infinite support equivalent of a circulant matrix.  The response of an LSI system can be therefore represented as The latter linear operation is called convolution and denotes as $h \\ast f$. It is straightforward to show that $h \\ast f = f \\ast h$. For real-valued functions, convolution of $h$ with $f$ can be also seen as $(h \\ast f)(\\bb{x}) = \\langle \\tau_{\\bb{x}} \\bar{h}, f  \\rangle$,where $\\bar{h}(\\bb{x}) = h(-\\bb{x})$. Fourier transform Harmonics A multi-dimensional harmonic is the function $\\phi_{\\bb{\\xi}}(\\bb{x}) = e^{2\\pi \\ii \\, \\bb{\\xi}^\\Tr \\bb{x}}$ with the $d$-dimensional real parameter $\\bb{\\xi}$. Note that the function has constant values along lines perpendicular to the direction of $\\bb{\\xi}$, and makes a full period every $1/| \\bb{\\xi} |$ units of space in the direction of $\\bb{xi}$. In other words,  $\\phi_{\\bb{\\xi}}$ looks like a periodic wave in the direction of $\\bb{\\xi}$ with the wavelength inversely proportional to $|\\bb{\\xi}|$ (or frequency proportional to $|\\bb{\\xi}|$). For this reason, $\\bb{\\xi}$ is usually referred to as the spatial frequency measured in inverse length units.Note that $\\phi_{\\bb{\\xi}}(\\bb{p}) = \\phi_{\\bb{p}}(\\bb{\\xi})$, meaning that we can freely exchange the roles of the space location $\\bb{x}$ and the spatial frequency $\\bb{\\xi}$. Fourier transform Orthogonal projections onto the set of harmonics is a function of the argument $\\bb{\\xi}$, is called the (forward) Fourier transform of $f$. We will denote it as the action of the operator $\\mathcal{F}$ on $f$ or simply by the capital $F$. The argument of $F$  is conventionally referred to as frequency, while the argument of $f$ is called space. The inverse Fourier transform is defined as In what follows, we prove several very useful properties of the Fourier transforms. Tensor products Let $f_1,\\dots, f_d : \\RR \\rightarrow \\RR$ be functions of a single variable, and let us define the tensor product $(f_1 \\otimes \\cdots \\otimes f_d)(\\bb{x}) = f_1(x_1) \\cdots f_d(x_d)$. Applying the Fourier transform yields We can write this result in the following convenient notation: A useful example is the $d$-dimensional box function that can be defined as a tensor product of one-dimensional rectangular functions:$\\mathrm{box} = \\mathrm{rect} \\otimes \\cdots \\otimes \\mathrm{rect}$, where Since $(\\mathcal{F} \\mathrm{rect})(\\xi) = \\mathrm{sinc}(\\xi) = \\frac{\\sin(\\pi \\xi)}{\\pi \\xi} $, we have $(\\mathcal{F} \\mathrm{box})(\\bb{\\xi}) =  \\mathrm{sinc}(\\xi_1) \\cdots \\mathrm{sinc}(\\xi_d)$. Translation and modulation Let $\\bb{p} \\in \\RR^d$ and $f : \\RR^d \\rightarrow \\RR$. Then, We can write this result as the duality between translation and modulation (i.e., multiplication by a harmonic) Obviously, the same relation holds when swapping the space and frequency domains.$\\phi_{\\bb{p}} f \\fff  \\tau_{\\bb{p}} F$.In other words, shift in the space domain results in modulation (the addition of linear phase) in the frequency domain and modulation in the space domain results in shift in the frequency domain. Convolution Let $\\bb{p} \\in \\RR^d$ and $f,h : \\RR^d \\rightarrow \\RR$. Then, In other words, convolution is dual to multiplication Obviously, the relation $h \\cdot f \\fff H \\ast F$ also holds. The fact that convolution (a Toeplitz operator) becomes a pointwise product (a diagonal operator) under the Fourier transform is the manifestation of the fact that the harmonics form an eigenbasis for every linear shift invariant system (or, equivalently, the Fourier transform diagonalizes every linear shift invariant operator). To see that, observe that when a harmonic $\\phi_{\\bb{\\xi}}$ is given as the input to a system described by the impulse response $h$, the output, meaning that $\\phi_{\\bb{\\xi}}$ is an eigenfunction of the system corresponding to the eigenvalue $F(\\bb{\\xi})$. Note that the spectrum is continuous and depends on the specific $h$, while the eigenfunctions are independent of $h$.\\footnote{It is a general fact from linear algebra that operators can be jointly diagonalized if and only if they commute with each other.} Stretching Let $\\bb{A} \\in \\RR^{d \\times d}$ be a regular (i.e., non-singular) matrix and let us define the stretching operator $\\mathcal{S}_{\\bb{A}} : f(\\bb{x}) \\mapsto f(\\bb{A}\\bb{x})$. Then, In other words, This identity will reappear when talking about dual lattices in relation to sampling. A useful example of the stretching property is the multi-dimensional Gaussian. Let us first remind ourselves (or accept without a proof) that in the one-dimensional case $e^{-x^2} \\fff e^{-\\pi \\xi^2}$. The tensor product property immediately yields $e^{-\\bb{x}^\\Tr \\bb{x} } \\fff e^{-\\pi \\bb{\\xi}^\\Tr \\bb{\\xi} }$ in the case of unit covariance (a.k.a. normal) multidimensional Gaussian. A general Gaussian $e^{-\\bb{x}^\\Tr \\bb{C}^{-1} \\bb{x} }$ with the covariance matrix $\\bb{C}$ can be obtained by stretching the normal Gaussian with the symmetric inverse square root matrix $\\bb{A} = \\bb{C}^{-\\frac{1}{2}}$, since $(\\bb{A} \\bb{x})^\\Tr (\\bb{A} \\bb{x}) = \\bb{x}^\\Tr \\bb{A}^\\Tr \\bb{A} \\bb{x} = \\bb{x}^\\Tr \\bb{C}^{-1} \\bb{x}$. The stretching property of the Fourier transform tells us that the corresponding stretch in frequency is by $\\bb{A}^{-\\Tr} = \\bb{C}^{\\frac{1}{2}}$ with furher scaling of the transform by the determinant of $\\bb{C}^{\\frac{1}{2}}$: The covariance matrix $\\bb{C}$ tells us how much the signal is concentrated in various spatial directions; its determinant can be used to quantify this spread: when $\\det \\bb{C}$ is small, the signal is localized in space. In the frequency domain, the signal remains Gaussian with the inverse covariance. Since both $\\bb{C}$ and $\\bb{C}^{-1}$ have the same eigenbasis but reciprocal eigenvalues, observe that the more the signal is concentrated in space in a certain direction, the more it is spread in frequency in the corresponding direction, and vice versa. The quantity $\\pi  \\det \\bb{C}^{-1}$ measures the spread of the signal in frequency. Since $\\det \\bb{C} \\cdot \\pi  \\det \\bb{C}^{-1} = \\pi$, we conclude that the Gaussian cannot be simultaneously localized both in space and frequency. This conclusion holds for every signal\\footnote{Actually, the Gaussian achieves the best possible simultaneous localization in both domains.} – a result known as the uncertainty principle with profound implications on why the Universe looks like it looks. Rotation An important particular case of the stretching property is when $\\bb{A}$ is an orthonormal matrix representing rotations (and, possibly, reflections). Let us define the rotation operator as $\\mathcal{R} _{\\bb{R}} : f(\\bb{x}) \\mapsto f(\\bb{R}\\bb{x})$, where $\\bb{R}$ is a rotation matrix. Since orthonormal matrices satisfy $\\bb{R}^{-1} = \\bb{R}^\\Tr$ and $ \\det \\bb{R} = \\pm 1$, the stretching property reduces to $\\mathcal{R} _{\\bb{R}} f \\fff \\mathcal{R} _{\\bb{R}} F$. In other words, the Fourier transform commutes with rotation: Projection Let us define the projection operator along the last spatial coordinate as Note that projection maps functions from $\\mathbb{F}(\\RR^d,\\RR)$ to $\\mathbb{F}(\\RR^{d-1},\\RR)$. Interpreting $f$ as a $d$-dimensional probability density function, the latter operation can be interpreted as marginalization with respect to $x_d$. Invoking the $(d-1)$-dimensional Fourier transform on $\\mathcal{P} f$ yields Defining the slice operator $Q : f(\\bb{x}) \\mapsto f(x_1,\\dots, x_{d-1},0)$ from $\\mathbb{F}(\\RR^d,\\RR)$ to $\\mathbb{F}(\\RR^{d-1},\\RR)$ we can express the latter result more compactly as $\\mathcal{F}\\mathcal{P} = \\mathcal{Q}\\mathcal{F}$. Note that while on the right hand side $\\mathcal{F}$ denotes a $d$-dimensional Fourier transform, on the left hand side it stands for the $(d-1)$-dimensional counterpart. Applying a rotation operator on the right to both sides of the identity yields where in the last passage we used the commutativity of the Fourier transform with rotation. We interpret the composition  $\\mathcal{P} \\mathcal{R} _\\bb{R}$ as a general projection operator,  $\\mathcal{P}  _{\\bb{R}}$, that first rotates the function and then project along the last axis. This essentially allows to project the function along any direction. In the same manner, we interpret $\\mathcal{Q} \\mathcal{R} _{\\bb{R}}$ as a general slice operator, $\\mathcal{Q} _{\\bb{R}}$, slicing the function along an arbitrary direction. This general result is known as the slice-projection theorem that in our notation can be expressed as An extremely important example where this result is used is computerized tomography (CT). In an idealized scenario, let us assume a $d=2$ dimensional world, in which we are interested in measuring the density of a slice of the human body, denoted by the function $f(x,y)$. Being unable to actually slice it (without going to jail), the next best thing we can do is to irradiate it with penetrating radiation (x-rays) from the side. Let us assume that an x-ray source sends parallel rays from one side of the body to the other side along the vertical ($y$) direction, where a linear detector measures the intensity profile $I(x)$. According to the Beer-Lambert law of light attenuation, the measured intensity is given by where $I_0$ is the emitted intensity. Taking the logarithm yields Rotating the emitter-detector setup around the body yields a collection of projections $\\mathcal{P} _\\theta f$ (note that in two dimensions, the rotation matrix is parameterized by a single angle $\\theta$). The function $(x,\\theta) \\mapsto (\\mathcal{P} _\\theta f)(x)$ is often referred to as the Radon transform or the sinogram of $f$. The slice projection theorem tells us that the one-dimensional Fourier transform of each such projection $\\mathcal{P} _\\theta f$ yields a correspondingly directed slice $\\mathcal{Q} _\\theta f$ of the two-dimension Fourier transform of the unknown function $f$. Collecting enough projections, it is ‘‘just’’ a matter of numerics to estimate the said transform and invert it, yielding what we see on the screen as a slice of a CT scan. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lecture_notes/lecture_1/",
        "teaser":null},{
        "title": "Lecture 2: Sampling and Interpolation",
        "excerpt":"Notation In our course, we will deal almost exclusively with real functions. Ascalar function will be denoted as $f : \\RR^d \\rightarrow \\RR$,$f({\\bm{\\mathrm{x}}})$, or simply $f$, with $d$ denoting the domaindimension. $d$-dimensional multi-indices will be denoted by${\\bm{\\mathrm{n}}} = (n_1,\\dots,n_d) \\in {\\bm{\\mathrm{Z}}}^d$. Anoperator acting on a function will be denoted by $\\mathcal{H}(f)$ orsimply $\\mathcal{H}f$. Dirac’s delta In what follows, we define a little bit of a technical machinery thatwill have crucial importance in the rigorous treatment of sampling. Adjoint operators We start with a little digression. Let $\\mathbb{U}$ and $\\mathbb{V}$ betwo spaces equipped with inner products, and let$\\mathcal{A} : \\mathbb{U} \\rightarrow \\mathbb{V}$ and$\\mathcal{B} : \\mathbb{V} \\rightarrow \\mathbb{U}$ be two operators. Iffor every $u \\in \\mathbb{U}$ and $v \\in \\mathbb{V}$ it holds that$\\langle \\mathcal{A}u, v \\rangle_{\\mathbb{V}} = \\langle u, \\mathcal{B} v \\rangle_{\\mathbb{U}}$,we will say that the operator $\\mathcal{B}$ is the adjoint of$\\mathcal{A}$ and denote it by $\\mathcal{B} = \\mathcal{A}^\\ast$. Adjointis the generalization of the notion of transpose (Hermitian transpose inthe complex case) of a matrix. An operator satisftying$\\mathcal{A}^\\ast  = \\mathcal{A}$ is said to be self-adjoint (think ofa symmetric matrix). Warning: it is tempting to confuse adjoint withinverse, as both are operators from $\\mathbb{V}$ to $\\mathbb{U}$. Theseare very different notions, coinciding only in the case of unitaryoperators. For example, the adjoint of the translation operator$\\tau_{\\bm{\\mathrm{p}}}$ can be immediately derived from from where $\\tau_{\\bm{\\mathrm{p}}}^\\ast = \\tau_{-\\bm{\\mathrm{p}}}$(verify this by thinking of $\\tau_{\\bm{\\mathrm{p}}}$ as of a Toeplitzmatrix and taking its transpose). As a more general example, consideroperator convolving the input function with a given real-valued function$h$. Then, from where the adjoint of $h \\ast f$ is $\\bar{h} \\ast f$, with$\\bar{h}({\\bm{\\mathrm{x}}}) = h(-{\\bm{\\mathrm{x}}})$. Yet another example: it is straightforward to see that the point-wiseproduct with a given real-valued function $h$ is self-adjoint, since Finally, let us derive the adjoint of the Fourier transform: We observe that $\\mathcal{F}^\\ast = \\mathcal{F}^{-1}$, implying that $\\mathcal{F}$is a unitary transformation (an isometry). From this geometricproperties, Plancherel’s identity follows: with$F = \\mathcal{F} f$ and $G = \\mathcal{F} g$. In the particular case of$g=f$, we obtain the celebrate Parseval’s identity, which is precisely what isometry means. We will see more adjoint operators in the sequel. Distributions Let $f : \\RR^d \\rightarrow \\RR$ be given function. A funny way torepresent it is to consider the action of the inner product with $f$ ona real-valued test function1 $\\psi$: Essentially, we defined a linear functional assigning each test functiona real scalar. Let us now think of a probability measure $\\mu$ on$\\RR^d$. A way to represent the measure is to consider its moments ofthe form  (we canthink of the previous example as a realization of the latter one if wethink of $f$ as the probability density function embodying $\\mu$).Again, a linear functional is used to represent a probability measure. A general linear functional assigning real scalars to tests functions iscalled a distribution. Because of our first example, we will denotethe action of such a functional on a test function using the innerproduct notation,  Beware: this isjust a convenient notation. Of course, $L$ is not a function and theabove inner product makes no sense. We simply define it to assume thevalue of $L(\\psi)$. The notation is convenient because it allows todefine in a consistent way what it means to apply an operator to adistribution. Let us return to our first example of the distribution$F(\\psi) = \\langle f, \\psi \\rangle$ representing a function $f$, and letus now be given an operator $\\mathcal{A}$. What is the meaning of“$\\mathcal{A}F$”? Rigorously, $F$ is not a function and the latterexpression makes no sense; yet, can we attribute it a meaning? In ourparticular case, we can interpret $\\mathcal{A}F$ as a new distribution We will now carry this trick with the adjoint to the general case: givena distribution $L$, we define Note that the left hand side is just convenient notation that rigorouslyspeaking has no mathematical sense. The right hand side, on the contraryis well-defined: no operator is acting on $L$; instead, the adjointoperator acts on the test function. This essentially gives rigoroussense and meaning to “$\\mathcal{A} L$”. With these understandings inmind, let us now introduce a very important distribution and study itsproperties. Delta distribution The distribution $\\delta(\\psi) = \\psi(0)$ is called the Dirac delta.Using the inner product notation, we can write While the second expression is just a syntactic sugar to $\\delta(\\psi)$,the third one is simply nonsensensical: $\\delta$ is not a function andcannot be integrated; furthermore, there exists no such a function theintegration with which would yield the value of $\\psi$ at${\\bm{\\mathrm{x}}} = 0$.2 ! Yet, engineering books are full with sucha notation, referring to $\\delta$ as to a “generalized function”. Using our definition of$(\\mathcal{A} \\delta)(\\psi) = \\delta(\\mathcal{A}^\\ast \\psi)$, we canreadily list a few important properties of the delta (it only takes toderive the adjoint operator). Translation Pointwise product Given a function $h : \\RR^d \\rightarrow \\RR$ and using the fact thatpointwise product with a real-valued function is self-adjoint, we obtain Convolution Given a function $h : \\RR^d \\rightarrow \\RR$ and using the fact that theadjoint of convolution with $h$ is the convolution with$\\bar{h}({\\bm{\\mathrm{x}}}) = h(-{\\bm{\\mathrm{x}}})$ yields Note that the right hand side is a distribution representing thefunction $h$. With some abuse of notation, we can say that theconvolution of $\\delta$ with a function $h$ ceases to be distributionand becomes the function $h$ itself. Delta is actually the onlydistribution acting as an identity element, a fact allowing to define agroup of functions with convolution serving as the group operation. Fourier transform Using the unitarity of the Fourier transform, where $\\mathbbl{1}({\\bm{\\mathrm{\\xi}}}) = 1$ is a constant function.Again, the last expression is a distribution representing the constantfunction $\\mathbbl{1}$, so we can write (with some abuse of notation):$ \\delta  \\fff \\mathbbl{1}$. Combining this result with thetranslation/modulation property leads to Stretching In order to see the action of the stretrching operator$\\mathcal{S}_{\\bm{\\mathrm{A}}} : f(\\bm{\\mathrm{x}}) \\mapsto f(\\bm{\\mathrm{A}}\\bm{\\mathrm{x}})$ on the delta, let us first derive its adjoint: from where$ {\\mathcal{S} _\\bm{\\mathrm{A}}}^\\ast = \\frac{ \\mathcal{S} _{\\bm{\\mathrm{A}}}^{-1} }{ | \\det {\\bm{\\mathrm{A}}} | } $. Invoking the definion of an operator acting on a distribution, Informally, thinking of $\\delta$ as of a “function”, we can write$\\delta({\\bm{\\mathrm{A}}}{\\bm{\\mathrm{x}}})  = \\frac{ \\delta({\\bm{\\mathrm{x}}}) }{| \\det {\\bm{\\mathrm{A}}} |}$.A straightforward corollary is that $\\delta$ is invariant to rotation. Derivatives In order to define delta’s derivatives, let us first examine the adjointof the derivative operator. For the sake of clairity, let us firstconsider the partial derivative with respect to the first coordinate,$\\partial_1 f = \\frac{\\partial}{\\partial x_1} f$. Using integration byparts yields By asserting that the functions $f$ and $g$ vanish at infinity, weobtain In the same manner, for any partial derivative operator of order $n$, parametrized by the multi-index ${\\bm{\\mathrm{n}}}$, it isstraightforward to show that$\\partial_{\\bm{\\mathrm{n}}}^\\ast = (-1)^{|\\bm{\\mathrm{n}}|} \\partial_{\\bm{\\mathrm{n}}}$,where $|\\bm{\\mathrm{n}}| = n_1 + \\cdots + n_d$. With this result, wecan now write Dirac’s bed Dirac’s bed Equipped with the delta and its basic properties, we are ready to definea new distribution that we will call a Dirac’s bed or a bed ofimpulses (on the integer lattice; we will generalize this definition inthe sequel): The importance of this distribution comes from the following twoproperties: Pointwise product Given a real-valued function $f$, Therefore, we can write Note that the resulting distribution is still a bed of impulses, but noweach impulse placed at point ${\\bm{\\mathrm{p}}}$ of the integer latticeis scaled by the value of the function $f$ at that point. This is aconvenient way to represent the action of sampling. Convolution As in the previous case, Note that the resulting distribution is no more a bed of impulses butrather the periodic function In other words, convolution with a bed of impulses is akin toperiodization of the function. Poisson summation identity Let $\\psi$ be a test function and let us define it periodized version As a periodic function, it can be represented as a ($d$-dimensional)Fourier series with Therefore, Substituting $\\bm{\\mathrm{x}} = \\bm{\\mathrm{0}}$, we obtain as well as This leads to the following amazing result known as the Poissonsummation formula: The following corollary follows directly: where we denote as usual $\\Psi = \\mathcal{F} \\psi$. In other words, Combining this result with the convolution/product duality, we obtainthe following duality between sampling and periodization: In other words, sampling the signal in the space domain result in theperiodization of the Fourier transform, and vice versa, periodization ofthe signal in the space domain results in a sampled Fourier transform.The latter is a mere justification of the fact that periodic functionscan be represented as Fourier series that have a discrete set of spatialfrequencies. The former leads to the famous sampling theorem. Sampling theorem Sampling a function on the integer lattice is descrbied by the followingdistribution Our previous result tells us that the Fourier transform of thisdistribution is the periodization of the Fourier transform$F({\\bm{\\mathrm{\\xi}}}) = (\\mathcal{F}f)({\\bm{\\mathrm{\\xi}}})$, It it happens that$\\mathrm{supp}(F) = {  {\\bm{\\mathrm{\\xi}}} : F({\\bm{\\mathrm{\\xi}}}) \\ne 0 } \\not\\subset \\left[-\\frac{1}{2},\\frac{1}{2}\\right]^d$,in the above summation, the shifted replicas of the Fourier transformsof $f$ will overlap forever losing the information contained in them andproducing originally inexistent content at lower frequencies. Thisphenomenon is called aliasing. On the other hand, if $\\mathrm{supp}(F) \\subset \\left[-\\frac{1}{2},\\frac{1}{2}\\right]^d$, theshifted replicas will not overlap and no information loss will occur. The original Fourier transform of $f$ can be recovered from $\\tilde{F}$by multiplying the latter with the box function taking the value of $1$on $\\left[-\\frac{1}{2},\\frac{1}{2}\\right]^d$ and $0$ elsewhere: . We have seen already the following duality  from which we obtain This result tells us that if the function $f$ has$\\mathrm{supp}(F) \\subset \\left[-\\frac{1}{2},\\frac{1}{2}\\right]^d$ (suchfunctions are called band limited), we can perfectly reconstruct itfrom its sampled version by means of the above series (which issometimes called the cardinal series), and is known as the(Kotelnikov-Whittaker-Nyquist-Hartley-Shannon) sampling theorem. In what follows, we extend this result to sampling on a general lattice. Lattices The sampling and periodization we dealt with so far was defined on theso-called integer lattice, $\\mathbb{Z}^d$. A more general lattice isdefined by means of a regular (=non-singular) linear transformation of$\\mathbb{Z}^d$: To perform sampling on $\\mathcal{L} = {\\bm{\\mathrm{A}}}\\mathbb{Z}^d$, wedefine Since , we have where by $\\mathrm{vol}\\, \\mathcal{L}$ we denote the volume of thestructural element of the lattice. Using the stretching property of the Fourier transform, we have Note that the Fourier transform of a bed of impulses on the lattice$\\bm{\\mathrm{A}}\\mathbb{Z}^d$ is proportional to a bed of impulses onthe lattice $\\bm{\\mathrm{A}}^{-\\Tr}\\mathbb{Z}^d$. Note that since$(\\bm{\\mathrm{A}}^{-\\Tr})^\\Tr \\bm{\\mathrm{A}} = \\bm{\\mathrm{A}}^{-1} \\bm{\\mathrm{A}} = \\bm{\\mathrm{I}}$,the columns of ${\\bm{\\mathrm{A}}}$ and ${\\bm{\\mathrm{A}}}^{-\\Tr}$ areorthonormal. It is convenient to define this particular lattice as thedual or reciprocal of $\\mathcal{L}$, Note that In this notation, we can write  Sampling theorem on a lattice Let ${\\bm{\\mathrm{A}}}$ be a regular matrix defining the lattice$\\mathcal{L} = \\bm{\\mathrm{A}} \\mathbb{Z}^d$. We will denote by$\\mathcal{L}_0 = \\bm{\\mathrm{A}}\\left[-\\frac{1}{2},\\frac{1}{2}\\right]^d$the structural element of the lattice, i.e., the smallest set suchthat We will say that a function $f$ is $\\mathcal{L}_0$-band limited if$ \\mathrm{supp}(F) \\subset\\mathcal{L}_0$. Let us denote${\\bm{\\mathrm{B}}} = {\\bm{\\mathrm{A}}}^{-\\Tr}$ and define$g(\\bm{\\mathrm{x}}) = f(\\bm{\\mathrm{B}} \\bm{\\mathrm{x}}) = (\\mathcal{S}_{\\bm{\\mathrm{B}}} f)(\\bm{\\mathrm{x}})$. According to the stretching property of the Fourier transform, Hence, Hence, $g$ is band-limited on $\\left[-\\frac{1}{2},\\frac{1}{2}\\right]^d$and we can write with${\\bm{\\mathrm{y}}} = {\\bm{\\mathrm{B}}}{\\bm{\\mathrm{x}}}$ In other words, a $\\mathcal{L}_0$-band limited $f$ can be perfectlyreconstructed from its samples on $\\mathcal{L}^\\ast$. Anti-aliasing Many real-world signals such as images are not truly band-limited(though do exhibit decaying spectrum). In order to avoid aliasing atsampling, the signal has to be made band-limited. If sampling occurs onlattice $\\mathcal{L}^\\ast$, the signal has to be $\\mathcal{L}$band-limited in order to allow perfect reconstruction. A real signal ismade band-limited by applying a low pass anti-aliasing filterimmediately prior to sampling. The filter has to be applied in theanalog domain – in electronics when sampling a one-dimensional timesignal, or in optics when sampling an image. A lens has a natural highfrequency cut-off due to diffraction limit, which acts as ananti-aliasing filter. The ideal anti-aliasing filter should have the frequency response$H_{\\mathcal{L}}(\\bm{\\mathrm{\\xi}})$ with $\\mathrm{supp}(H_{\\mathcal{L}}) \\subset \\mathcal{L_0}$, which is achieved by$H_{\\mathcal{L}}(\\bm{\\mathrm{\\xi}}) = \\mathrm{box}( \\bm{\\mathrm{A}}^{-1} \\bm{\\mathrm{\\xi}} )$.Using the stretching property of the Fourier transform, we obtain theimpulse response of the filter, Real anti-aliasing filters are never ideal, therefore some amount ofaliasing is always present in real systems. Interpolation Sampling theorem tells us that when a function $f$ is band-limited onthe lattice $\\mathcal{L} = {\\bm{\\mathrm{A}}} \\mathbb{Z}^d$ (that is,$ \\mathrm{supp}(F) \\subset {\\bm{\\mathrm{A}}}\\left[-\\frac{1}{2},\\frac{1}{2}\\right]^d$),it can be perfectly reconstructed from its samples$f|_{\\mathcal{L}^\\ast}$ on the dual lattice$\\mathcal{L}^\\ast = {\\bm{\\mathrm{A}}}^{\\ast} \\mathbb{Z}^d$, where${\\bm{\\mathrm{A}}}^\\ast = {\\bm{\\mathrm{A}}}^{-\\Tr}$, by the followingformula Let us examine the expression Remembering that $\\mathrm{sinc}(0) = 1$ and $\\mathrm{sinc}(n) = 0$ forevery $n \\in \\mathbb{Z} \\setminus { 0 }$, we obtain that for every${\\bm{\\mathrm{p}}} \\in \\mathcal{L}^\\ast$, we can write${\\bm{\\mathrm{p}}} = {\\bm{\\mathrm{A}}}^\\ast {\\bm{\\mathrm{k}}}$ for${\\bm{\\mathrm{k}}} \\in \\mathbb{Z}^d$, obtaining This means that on the points of the lattice $\\mathcal{L}^\\ast$, theinterpolation formula yields exactly the sampled values. Interpolation kernels The problem with the sinc is that it has infinite support. In realsystems, the interpolation formula takes place in an analog piece ofhardware (e.g., electronics or optics), which is limited to simplerfunctions with finite support. A real interpolation formula is thereforea revisited version of its ideal counterpart, where $p$ is a $d$-dimensional interpolation or reconstructionkernel| having the property that $p(\\bm{\\mathrm{0}}) = 1$ and$p|_{\\mathbb{Z}^d \\setminus {  \\bm{\\mathrm{0}}} } = 0$. In practice,a separable kernel $p(\\bm{\\mathrm{x}}) = p(x_1)\\cdots p(x_d)$ isoften used. The one-dimensional kernel $p$ again has to satisfy$p(0) = 1$ and $p(n)=0$ for $n \\in \\mathbb{Z} \\setminus {0}$. Thefollowing interpolation kernels are frequently used:       Zeroth-order or nearest-neighbor interpolation:$p(x) = \\mathrm{rect}(x)$. In case of one-dimensional signals, acausal version of this kernel,$p(x) = \\mathrm{rect}\\left(x-\\frac{1}{2}\\right)$ is used.Zeroth-order interpolation sets the value of the continuous-domainfunction to the value of the nearest (causal in the latter case)discrete sample.         First-order or (multi-) linear interpolation:$p(x) =  \\max (  1 - |x|, 0 )$. First-order interpolation continuesthe function between the samples linearly, essentially connectingthe samples by lines (hyperplanes in the $d$-dimensional case).Bi-linear interpolation is the particular case for $d=2$.         Third-order or (multi-) cubic interpolation:         The parameter $a$ is usually set to $a=-0.5$or $a=-0.75$. Cubic interpolation essentially fits a cubic Hermitespline to the data. Bi-cubic interpolation is the particular casefor $d=2$.         Lanczos interpolation:         The parameter $a$ is a positive integer,typically 2 or 3, which determines the size of the kernel. TheLanczos kernel has $2a − 1$ lobes: a positive one at the center, and$a − 1$ alternating negative and positive lobes on each side. Aslong as the parameter $a$ is a positive integer, the Lanczos kernelis continuous everywhere, and its derivative is defined andcontinuous everywhere (including $x = \\pm a$, where both sincfunctions go to zero). Therefore, the reconstructed signal too willbe $\\mathcal{C}^1$.               Technically, test functions must be sufficiently well-behaved. Wewill not dwell on these subtleties; in practice, any smoothcompactly supported function will suffice for our purpose. &#8617;               In finite-dimensional spaces, the space of linear functionals isisomorphic to the vector space itself, and distributions are just afunny way to define vectors through their inner products with testvectors. On the other hand, in infinitely-dimensional spaces, suchas our spaces of functions, no such isomorphism exists.Consequently, every function can be described as a distribution, butthere are distributions such as the $\\delta$ that cannot bedescribed by a function. It can be approximated to an desiredaccuracy by a sequence of functions, but the limit point of thesequence is not a function. &#8617;       ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lecture_notes/lecture_2/",
        "teaser":null},{
        "title": "Lecture 3: Discrete-domain signals and systems",
        "excerpt":"Discrete-domain Fourier transform When a function $f$ is sampled on the lattice $\\mathcal{L}^\\ast$, itsFourier transform is periodized on the reciprocal lattice $\\mathcal{L}$.Therefore, it is sufficient to describe the value of the transform onthe structural element of the lattice, which can be parametrized as$\\bb{\\xi} = \\bb{A} \\bb{\\omega}$ with$\\bb{\\omega} \\in \\left[ -\\frac{1}{2}, \\frac{1}{2} \\right]$. The vector$\\bb{\\omega}$ is called the normalized or digital frequency. Substituting the normalized frequency into the periodized and scaledFourier transform of $f$ yields where we defined $f[\\bb{n}] =  f( \\bb{A}^\\ast \\bb{n})$. In what follows,with some abuse of notation, the discrete sequence of samples,$f| _{\\mathcal{L}^\\ast} = { f _{\\bb{n}} = f[\\bb{n}] } _{ \\bb{n} \\in \\mathbb{Z}^d }$,as well as its particular element will be denoted simply as $f _{\\bb{n}}$or $f[\\bb{n}]$. Square brackets will emphasize that we are working inthe digital (i.e., discetely sampled) domain. We will refer to$f[\\bb{n}]$ as to a discrete signal and to $F[\\bb{\\omega}]$ as to its(discrete-domain) Fourier transform. By convention, $F[\\bb{\\omega}]$is extended periodically to entire $\\mathbb{R}^d$. Note that, as before, the Fourier transform is defined as the orthogonalprojection, only this time the inner product is on the space of sequences The inverse transform is given by Since the discrete-domain Fourier transform is defined through its(periodized) continuous counterpart, all the properties of the latterare straightforwardly inherited. Discrete systems Let $f$ be a continuous-domain signal, $h$ the impulse response of anLSI system, and let us denote $g = f \\ast h$. Sampling $g$ on thelattice $\\mathcal{L}^\\ast = \\bb{A}^\\ast \\mathbb{Z}^d$ yields If either $f$ or $h$ is $\\mathcal{L} _0$-band limited, so is $f \\ast h$and we can further write$\\bed _{\\mathcal{L}} \\ast (F \\cdot H) = (\\bed _{\\mathcal{L}} \\ast F) \\cdot (\\bed _{\\mathcal{L}} \\ast H)$,hence On the other hand, Hence, we arrive at a discrete form of convolution It is straightforward to show that convolution is transformed topoint-wise product in the frequency domain. Kroenecker’s delta Recall that Dirac’s delta was a distribution that acted like identityunder convolution. The discrete-domain analog is the Kroenecker delta,which is the sequence $\\delta[\\bb{n}]$ defined as $\\delta[\\bb{0}]=1$ and$\\delta[\\bb{n}]=0$ for every $\\bb{n} \\ne \\bb{0}$. Decimation Let $f$ be a signal sampled on the lattice$\\mathcal{L}^\\ast = \\bb{A} \\mathbb{Z}^d$. From the definition of theFourier transform, the following relation holds between the discrete andcontinuous domains: Let $\\bb{M}^\\ast$ be an integer $d \\times d$ matrix such that$\\mathcal{M}^\\ast = \\bb{M}^\\ast \\mathbb{Z}^d \\subset \\mathbb{Z}^d$ (thatis, the lattice $\\mathcal{M}^\\ast$ is has only integer points). Wedenote by $\\mathcal{M}^\\ast _0$ the (discrete) structural element of$\\mathcal{M}^\\ast$, i.e., the smallest set such that Let us sub-sample $f[\\bb{n}]$ on $\\mathcal{M}^\\ast$ creating a newdiscrete signal$g[\\bb{n}] = f[\\bb{M}^\\ast\\bb{n}] = f(\\bb{A}^\\ast \\bb{M}^\\ast \\bb{n})$(this operation is ofter referred to as compression). Essentially,$g[\\bb{n}]$ can be thought of $f(\\bb{x})$ sampled on the dilated lattice$\\mathcal{L}^{\\prime \\ast} = \\bb{A}^\\ast \\bb{M}^\\ast \\mathbb{Z}^d $(with the corresponding dual$\\mathcal{L}^{\\prime} = \\bb{A} \\bb{M} \\mathbb{Z}^d$, where$\\bb{M}^\\ast = \\bb{M}^{-\\Tr}$). Using definitions and elementaryalgebra, we obtain the following expression for the Fourier transform of$g[\\bb{n}]$: Firstly, notice that the continous-domain Fourier transform of $f$ doesnot appear in the final expression; $G[\\bb{\\omega}]$ is entirelyexpressed in terms of the discrete-domain Fourier transform$F[\\bb{\\omega}]$. Therefore, we do not need to assume any underlyingcontinuos-domain signal $f(\\bb{x})$ (though it can always be obtained atleast in principle via band-limited interpolation of $f[\\bb{n}]$), andperform the sub-sampling of $f[\\bb{n}]$ entirely in the discrete domain. Secondly, observe the striking resemblance of the above expression toits continuous counterpart. The discrete-domain Fourier transform of thesignal being sampled is periodized on the dual lattice $\\mathcal{M}$(recall that sub-sampling is performed on $\\mathcal{M}^\\ast$). As in thecontinuous case, the support of $F[\\bb{\\omega}]$ has to be containedwithin the structural element of the dual lattice $\\mathcal{M}$ in orderto avoid aliasing; in other words, $f[\\bb{n}]$ has to be$\\mathcal{M}$-band limited. In that case, the replicas of$F[\\bb{\\omega}]$ will not overlap, and $f[\\bb{n}]$ can be recoveredexactly from $g[\\bb{n}]$ by keeping only the main replica of$F[\\bb{\\omega}]$ in $G[\\bb{\\omega}]$ and suppressing the others. Anti-aliasing In general, when the signal is not band-limited and an anti-aliasingfilter has to be applied before sub-sampling. The frequency responseof the ideal low-pass filter $H _{\\mathcal{M}}[\\bb{\\omega}]$ has tosatisfy $\\mathrm{supp}(H _{\\mathcal{M}}) \\subset \\mathcal{M} _0$ (ofcourse, this is a slightly sloppy notation: $H _{\\mathcal{M}}$ isperiodic, hence the support is replicated on $\\mathcal{M}$). Such alow-pass filter is achieved by$H _{\\mathcal{M}}[\\bb{\\omega}] = \\mathrm{box}( \\bb{M}^{-1} \\bb{\\omega} )$(again, periodized on $\\mathcal{M}$), whose impulse response isstraightforwardly obtained by invoking the stretching property of theFourier transform The combined action of digital anti-aliasing filtering followed bysub-sampling is usually referred to as decimation. On block diagrams,we will denote sub-sampling as $\\downarrow _{\\bb{M}^\\ast}$ or$\\bb{M}^\\ast : \\bb{I}$, followed by a decimation filter$H _{\\mathcal{M}}$. Interpolation Let $f$ be a signal sampled, as before, on $\\mathcal{L}^\\ast $, and let$\\bb{N}^\\ast$ be a $d \\times d$ matrix such that$\\mathbb{Z}^d \\subset \\mathcal{N}^\\ast = \\bb{N}^\\ast \\mathbb{Z}^d $(that is, a sub-sample of the lattice $\\mathcal{N}^\\ast$ is the integerlattice). We define the expansion of $f[\\bb{n}]$ to $\\mathcal{N}^\\ast$as Substituing $\\bb{p} = \\bb{N}^\\ast \\bb{n}$ and,inversely, $\\bb{n} = \\bb{N}^\\Tr \\bb{p}$, we obtain (observe the resemblance to the continuous case). In the frequencydomain, this translates to Since $\\mathrm{vol}\\, \\mathcal{N}^\\ast &lt; 1$, the periodic discrete-timeFourier transform $F[\\bb{\\omega}]$ is shrunk, and a few of its periodsalias to higher frequencies of $G[\\bb{\\omega}]$. However, unlike thecase of sub-sampling where information was lost, all the informationabout $f[\\bb{n}]$ is still present in $g[\\bb{n}]$. In order to fullyrecover it, we just need to remove the superfluous replicas of$F[\\bb{\\omega}]$ outside$\\bb{N}^{-1} \\left[ -\\frac{1}{2}, \\frac{1}{2} \\right]^d$. The frequencyresponse of the ideal low-pass filter $H _{\\mathcal{N}}[\\bb{\\omega}]$ hasto satisfy$\\mathrm{supp}(H _{\\mathcal{N}}) \\subset \\bb{N}^{-1} \\left[ -\\frac{1}{2}, \\frac{1}{2} \\right]^d$. Denoting $\\mathcal{L}^{\\prime} = \\bb{A} \\bb{N} \\mathbb{Z}^d$, bydefinition of the discrete-domain Fourier transform, since $H _{\\mathcal{N}}(\\bb{\\omega})$ removes all replicas of $F$except those with integer $\\bb{N}^{-1}\\bb{n}$. Note that we also assumedthat the filter has DC gain of $\\mathrm{vol}\\, \\mathcal{N}$ to make theresult consistent with the sampling of $f(\\bb{x})$ on$\\mathcal{L}^{\\prime \\ast}$. Such a filter is achieved by$H _{\\mathcal{N}}(\\bb{\\omega}) =  \\mathrm{vol}\\, \\mathcal{N} \\, \\mathrm{box}( \\bb{N} \\bb{\\omega} )$.The corresponding impulse response is Convolving $h _{\\mathcal{N}}[\\bb{n}]$ with $g[\\bb{n}]$ yields thefollowing identity The combined action of expansion followed by anti-aliasing filtering iscalled interpolation. Note the striking resemblance to the continuouscase. As in the continuous case, here as well practice mandates the useof other interpolation kernels that have finite support, which in thefrequency domain is translated to non-ideal low-pass filters. On blockdiagrams, we will denote expansion by $\\uparrow _{\\bb{N}^\\ast}$ or$\\bb{I} : \\bb{N}^\\ast$, followed by an interpolation filter$H _{\\mathcal{N}}$. Resampling Let $f$ be a signal sampled, as before, on $\\mathcal{L}^\\ast $, and let$\\mathcal{Q}^\\ast = \\bb{Q}^\\ast \\mathbb{Z}^d$ be a new lattice ontowhich we would like to resample $f$ without leaving the digitaldomain. If $\\bb{Q}^\\ast$ can be factorized into$\\bb{Q}^\\ast = \\bb{M}^\\ast \\bb{N}^\\ast$ such that$\\mathbb{Z}^d \\subset  \\bb{N}^\\ast \\mathbb{Z}^d$ and$\\bb{M}^\\ast \\mathbb{Z}^d \\subset \\mathbb{Z}^d$, we can firstinterpolate $f[\\bb{n}]$ to $\\bb{N}^\\ast \\mathbb{Z}^d$ and then decimateit onto $\\bb{M}^\\ast \\mathbb{Z}^d$ as described above. On block diagramswe will denote resamplers as $\\bb{M}^\\ast : \\bb{N}^\\ast$. They areparticularly useful when resampling images to a new grid related to theoriginal one by a non-integer (yet, rational) factor. A few elementary rules can be straightforwardly proved:       Successive sub-sampling cascades can be combined:$\\downarrow _{\\bb{M}^\\ast _1} \\longrightarrow\\, \\downarrow _{\\bb{M}^\\ast _2} \\, =\\,  \\downarrow _{\\bb{M} _2^\\ast \\bb{M} _1^\\ast}$         Successive expansion cascades can be combined:$\\uparrow _{\\bb{N}^\\ast _1} \\longrightarrow\\, \\uparrow _{\\bb{N}^\\ast _2} \\, =\\, \\uparrow _{\\bb{N} _2^\\ast \\bb{N} _1^\\ast}$         Expansion can be exactly inverted:$\\uparrow _{\\bb{N}^\\ast} \\longrightarrow\\, \\downarrow _{\\bb{N}^{-\\ast}} \\, =\\, \\mathrm{id}$         Sub-sampling cannot usually be exactly inverted:$\\downarrow _{\\bb{M}^\\ast} \\longrightarrow\\, \\uparrow _{\\bb{M}^{-\\ast}} \\, \\ne \\, \\mathrm{id}$   Noble identities Treating sub-sampling and expansion and discrete-domain systems, let usquickly highlight several of their properties. Linearity Observe that the sub-sampling operation we defined,$(\\downarrow _{\\bb{M}^\\ast} f)[\\bb{n}] = f[\\bb{M}^\\ast \\bb{n}]$ islinear, i.e., it commutes with addition and scaling: The same is also true for the expansion operation since Translation When changing the order of translation and sub-sampling, the latticematrix multiplies the translation vector: Similarly, for the expansion operation, Note that since the translation operator is modified when swapped withthe sub-sampling/expansion, these systems are linear but not shiftinvariant! Exchanging sub-sampling and filtering Let $\\mathcal{H}$ be an LSI system defined by Combining linearity and translation properties of sub-sampling, weobtain In other words, applying the system $\\mathcal{H}$ followed bysub-sampling on $\\mathcal{M}^\\ast$ is equivalent to first sub-samplingon $\\mathcal{M}^\\ast$ and then applying the system which can be described by the expanded impulse response The frequency response of this equivalent system isstraightforwardly given by This result is usually known as a noble identity and can be summarizedas or, alternatively, as the following informal expression where with some abuse of notation we represent the action of an LSIsystem by its Fourier transform. Exchanging expansion and filtering Similarly to the previous case, combining linearity and translationproperties of expansion, we obtain with that can be described by the expanded impulse response The frequency response of this equivalent system isstraightforwardly given by This yields our second noble identity that can be summarized as or, alternatively, as the following informal expression Polyphase representation Let us fix some lattice$\\mathcal{M}^\\ast = \\bb{M}^\\ast \\mathbb{Z}^d \\subset \\mathbb{Z}^d$ anddenote by $\\mathcal{M}^\\ast _0$ its structural element. In this notation,we can represent Let $\\mathcal{H}$ be an LSI system with the impulse response $h[\\bb{n}]$that can be described as We define the polyphase components of $h[\\bb{n}]$ with respect to$\\mathcal{M}^\\ast$ as for every $\\bb{m} \\in \\mathcal{M}^\\ast _0$. Each such component can be regarded asthe impulse response of a system in its own right, In these terms, the system $\\mathcal{H}$ itself can be expressed as Note that the inner sum is just an expanded impulse response of$\\mathcal{E} _{\\bb{m}}$, which in the frequency domain assumes the form of This decomposition of an LSI system in terms of the polyphase componentsis known as the type I polyphase decomposition. Alternatively, we can define the polyphase sequences$r _{\\bb{m}}[\\bb{n}] = e _{-\\bb{m}}[\\bb{n}] = h[\\bb{M}^\\ast \\bb{n} - \\bb{m}]$,yielding the type II polyphase decomposition: Efficient implementation of decimators When $\\mathcal{H}$ is a system implementing a decimation filter, it isfollowed by the sub-sampling $\\downarrow _{\\bb{M}^\\ast}$. Substitutingtype I polyphase decomposition with respect to $\\mathcal{M}^\\ast$ andapplying the noble identity yields or in operator writing Both sides of the identity can be viewed as two differentimplementations of the decimator system. Let us compare the computational complexity of these twoimplementations. For simplicity, let us assume $\\bb{M}^\\ast = M \\bb{I}$,so that $\\mathcal{M}^\\ast _0 = {0,1,\\dots,M-1}^d$, and furthermore, letus assume that $h[\\bb{n}]$ is supported on the discrete square${0,\\dots,K-1}^d$, while the input signal is supported on${0,\\dots,N-1}^d$. To simplify expressions, let us further assume thatboth $N$ and $K$ are divisible by $M$. The first system first calculated$N^d$ outputs of the filter $\\mathcal{H}$ (containing $K^d$coefficients), each of which requiring $K^d$ multiply-and-accumulate(MACC) operations. This totals in $(KN)^d$ MACC operations. If the inputsignals are images coming at a rate of one image per second, thisrequires a processor running at $(KN)^d$ hertz. For $d=2$, $M=4$, $K=1$,and $N=4000$ (a $16$ megapixel image decimated $\\times$ in each axisusing a $4 \\times 4$ bicubic kernel), this amounts to $256$ MHz! Notethat afterwards, only $1/16$-th of these output samples are retained bythe sub-sampler; the rest are thrown away. The second implementation first filters $M^d$ shifted and sub-sampledversions of the input signal (containing$\\displaystyle{ \\left(\\frac{N}{M}\\right)^d }$ samples) with the filters$\\mathcal{E} _{\\bb{m}}$ (containing$\\displaystyle{ \\left(\\frac{K}{M}\\right)^d }$ coefficients). Each filteroutput sample requires only$\\displaystyle{ \\left(\\frac{KN}{M^2}\\right)^d }$ MACC operations perfilter, and$\\displaystyle{ M^d \\left(\\frac{KN}{M^2}\\right)^d = \\left(\\frac{KN}{M}\\right)^d}$MACC operations in total. Note the save up by a factor of $M^d$. For thesame settings as before, we would need a processor running only at $16$MHz. Also note that the $M^d$ filters can be computed in parallel on$M^d$ cores running at $M^{-2d}$ the frequency of the core required toimplement the first system. In our example, if the processor had $16$cores, we could run each core at $1$ MHz. Since processor powerconsumption is roughly proportional to $f^3$ (or worse), the secondsystem requires roughly $M^{5d}$ less power. In our example, $16$$1$-MHz cores would consume about one million times less power. For thisreason, it is not an exaggeration to say that any ASIC implementingsignal decimation uses the polyphase implementation. Efficient implementation of interpolators We will now repeat the same analysis for the case when $\\mathcal{H}$ isan interpolation filter preceding expansion $\\uparrow _{\\bb{N}^\\ast}$.Substituting the type II polyphase representation with respect to$\\mathcal{N}^{-\\ast}$ and applying the noble identity yields or in operator writing As before, both sides of the identity can be viewed as two differentimplementations of the interpolator system. The second system firstfilters the inputs with the smaller filters $\\mathcal{R} _{\\bb{m}}$, theexpands the results and sums their shifted versions. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lecture_notes/lecture_3/",
        "teaser":null},{
        "title": "Lecture 4: Random signals",
        "excerpt":"Elementary probability Probability measure We start with a few elementary (and simplified) definitions from thetheory of probability. Let us fix a sample space $\\Omega = [0,1]$. ABorel set on $\\Omega$ is a set that can be formed from open intervalsof the form $(a,b), 0 \\le a&lt;b \\le 1$, through the operations ofcountable union, countable intersection, and set difference. We willdenote the collection of all Borel sets in $\\Omega$ as $\\Sigma$. It ispretty straightforward to show that $\\Sigma$ contains the empty set, isclosed under complement, and is closed under countable union. Such a setis known as $\\sigma$-algebra and its elements (subsets of$\\mathbb{R}$) are referred to as events. A probability measure $P$ on $\\Sigma$ is a function$P : \\Sigma \\rightarrow [0,1]$ satisfying $P(\\emptyset) = 0$,$P(\\mathbb{R}) = 1$ and additivity for every countable collection${ E _n \\in \\Sigma }$, Random variables A random variable $\\mathpzc{X}$ is a measurable map$\\mathpzc{X} : \\Omega \\rightarrow \\mathbb{R}$, i.e., a function suchthat for every $a$,${ \\mathpzc{X} \\le a } = { \\alpha : \\mathpzc{X}(\\alpha) \\le a  } \\in \\Sigma$.The map $\\mathpzc{X}$ pushes forward the probability measure $P$; thepushforward measure $\\mathpzc{X} _\\ast P$ is given by where$\\mathpzc{X}^{-1}(A) = { \\alpha  : X(\\alpha) \\in A }$ is the preimageof $A \\subseteq \\mathbb{R}$. (In short, we can write$\\mathpzc{X} _\\ast P = P\\mathpzc{X}^{-1}$). This pushforward probabilitymeasure $\\mathpzc{X} _\\ast P$ is usually referred to as the probabilitydistribution (or the law) of $\\mathpzc{X}$. When the range of $\\mathpzc{X}$ is finite or countably infinite, therandom variable is called discrete and its distribution can bedescribed by the probability mass function (PMF): which is a shorthand for $P(  {\\alpha : \\mathpzc{X}(\\alpha) = x } )$. Otherwise, $\\mathpzc{X}$ is called a continuous random variable.Any random variable can be described by the cumulative distribution function (CDF) which is a shorthand for$F _{\\mathpzc{X}}(x) = P(  {\\alpha : \\mathpzc{X}(\\alpha) \\le x } )$. If$X$ is absolutely continuous, the CDF can be described by the integral where the integrand $f _{\\mathpzc{X}}$ is known as the probability densityfunction (PDF)1. Uniform distribution and uniformization A random variable $\\mathpzc(U)$ is said to be uniformly distributed on$[0,1]$ (denoted as $\\mathpzc{U} \\sim \\mathcal{U}[0,1]$) if In other words, the map $\\mathpzc{U}$ pushes forward the standard Lebesgue measure on$[0,1]$, $\\mathpzc{U} _\\ast P = \\lambda$. The corresponding CDF is$F _\\mathpzc{U}(u) = \\max{ 0, \\min{ 1, u } }$. Let $\\mathpzc{X}$ besome other random variable characterized by the CDF $F _\\mathpzc{X}$. Wedefine $\\mathpzc{U} = F _\\mathpzc{X}(\\mathpzc{X})$. Let us pick anarbitrary $x \\in \\mathbb{R}$ and let $u = F _\\mathpzc{X}(x) \\in [0,1]$.From monotonicity of the CDF, it follows that $\\mathpzc{U} \\le u$ if andonly if $\\mathpzc{X} \\le x$. Hence,$F _\\mathpzc{U}(u) = P(\\mathpzc{U} \\le u) = P(\\mathpzc{X} \\le x) = F _\\mathpzc{X}(x) = u$.We conclude that by transforming a random variable with its own CDFuniformizes it on the interval $[0,1]$. Applying the relation in inverse direction, let$\\mathpzc{U} \\sim \\mathcal{U}[0,1]$ and let $F$ be a valid CDF. Then,the random variable $\\mathpzc{X} = F^{-1}(\\mathpzc{U})$ is distributedwith the CDF $F _\\mathpzc{U} = F$. Expectation The expected value (a.k.a. the expectation or mean) of a randomvariable $\\mathpzc{X}$ is given by where the integral is the Lebesgue integral w.r.t. the measure $P$;whenever a probability density function exists, the latter can bewritten as Note that due to the linearity of integration, the expectation operator$\\mathbb{E}$ is linear. Using the Lebesgue integral notation, we canwrite for $E \\in \\Sigma$ where is the indicator function of $E$, which is by itself arandom variable. This relates the expectation of the indicator of anevent to its probability. Moments For any measurable function $g : \\mathbb{R} \\rightarrow \\mathbb{R}$,$\\mathpzc{Z} = g(\\mathpzc{X})$ is also a random variable with theexpectation Such an expectation is called a moment of $\\mathpzc{X}$. Particularly,the $k$-th order moment is obtained by setting $g(x) = x^k$, The expected value itself is the first-order moment of $\\mathpzc{X}$, which is oftendenoted simply as $\\mu _\\mathpzc{X} = \\mu _{1}(\\mathpzc{X})$. Thecentral $k$-th order moment is obtained by setting$g(x) = (x - \\mu _\\mathpzc{X})^k$, A particularly important central second-order moment is the variance Joint distribution A vector $\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ ofrandom variables is called a random vector. Its probabilitydistribution is defined as before as the pushforward measure$P = \\mathpzcb{X} _\\ast \\lambda$ Its is customary to treat $\\mathpzcb{X}$as a collection of $n$ random variables and define their joint CDF as As before, whenever the following holds the integrand $f _{\\mathpzcb{X}}$ is called the joint PDF of$\\mathpzcb{X}$. The more rigorous definition as the Radon-Nikodymderivative stays unaltered, only that now $\\lambda$ is the $n$-dimensional Lebesguemeasure. Marginal distribution Note that the joint CDF of the sub-vector$(\\mathpzc{X} _2, \\dots, \\mathpzc{X} _n)$ is given by Such a distribiution is called marginal w.r.t. $\\mathpzc{X} _1$ and theprocess of obtaining it by substituting $x _1 = \\infty$ into$F _{\\mathpzcb{X}}$ is called marginalization. The corresponding actionin terms of the PDF consists of integration over $x _1$, Statistical independence A set $\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n$ of random variables iscalled statistically independent if their joint CDF iscoordinate-separable, i.e., can be written as the following tensorproduct An alternative definion can be given in terms of the PDF (whenever itexists): We will see a few additional alternative definitions in the sequel. Convolution theorem Let $\\mathpzc{X}$ and $\\mathpzc{Y}$ be statistically-independent randomvariables with a PDF and let $\\mathpzc{Z} = \\mathpzc{X}+\\mathpzc{Y}$.Then, where we changed the variable $x$ to $x’ = x+y$. Differentiating w.r.t.$z$ yields Since $\\mathpzc{X}$ and $\\mathpzc{Y}$ are statistically-independent, wecan substitute$f _{\\mathpzc{X}\\mathpzc{Y}} = f _{\\mathpzc{X}} \\otimes f _{\\mathpzc{Y}}$yielding Limit theorems Given independent identically distributed (i.i.d.) variables$\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n$ with mean $\\mu$ and variance$\\sigma^2$, we define their sample average as Note that $\\mathpzc{S} _n$ is also a random variable with$\\mu _{\\mathpzc{S} _n} = \\mu$ and$\\displaystyle{\\sigma^2 _{\\mathpzc{S} _n} = \\frac{\\sigma^2}{n}}$. It isstraightforward to see that the variance decays to zero in the limit$n \\rightarrow \\infty$, meaning that $\\mathpzc{S} _n$ approaches adeterministic variable $\\mathpzc{S} = \\mu$. However, a much strongerresult exists: the (strong) law of large numbers states that in thelimit $n \\rightarrow \\infty$, the sample average converges inprobability to the expected value, i.e., This fact is often denoted as$\\mathpzc{S} _n \\mathop{\\rightarrow}^P \\mu$. Furthermore, defining thenormalized deviation from the limit$\\mathpzc{D} _n = \\sqrt{n}(\\mathpzc{S} _n - \\mu)$, the central limittheorem states that $\\mathpzc{D} _n$ converges in distribution to$\\mathcal{N}(0,\\sigma^2)$, that is, its CDF converges pointwise to thatof the normal distribution. This is often denoted as$\\mathpzc{D} _n \\mathop{\\rightarrow}^D \\mathcal{N}(0,\\sigma^2)$. A slightly more general result is known as the delta method instatistics: if $g :  \\mathbb{R} \\rightarrow \\mathbb{R}$ is a$\\mathcal{C}^1$ function with non-vanishing derivative, then by theTaylor theorem, where $\\nu$ lies between $\\mathpzc{S} _n$ and $\\mu$. Since by the law oflarge numbers $\\mathpzc{S} _n \\mathop{\\rightarrow}^P \\mu$, we also have$\\nu \\mathop{\\rightarrow}^P \\mu$; since $g’$ is continuous,$g’(\\nu) \\mathop{\\rightarrow}^P g’(\\mu)$. Rearranging the terms andmultiplying by $\\sqrt{n}$ yields from where (formally, by invoking the Slutsky theorem): Joint moments Given a measurable function$\\bb{g} : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, a (joint) moment of arandom vector $\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ is the last term migh be undefined if the PDF does not exist.The mean of a random vector is simply$\\bb{\\mu} _\\mathpzcb{X}  = \\mathbb{E} \\mathpzcb{X}$. Of particularimportance are the second-order joint moments of pairs of randomvariables, and its central version The latter quantity is known as the covariance of $\\mathpzc{X}$ and$\\mathpzc{Y}$. Two random variables $\\mathpzc{X}$ and $\\mathpzc{Y}$ with$r _{\\mathpzc{X}\\mathpzc{Y}} = 0$ are called orthogonal2 The variables with $\\sigma^2 _{\\mathpzc{X}\\mathpzc{Y}} = 0$ are calleduncorrelated. Note that for a statistically independent pair$(\\mathpzc{X},\\mathpzc{Y})$, However, the converse is not true, i.e., lack of correlation does notgenerally imply statistical independence (with the notable exception ofnormal variables). If $\\mathpzc{X}$ and $\\mathpzc{Y}$ are uncorrelatedand furthermore one of them is zero-mean, then they are also orthogonal(and the other way around). In general, the correlation matrix of a random vector$\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ is given by its $(i,j)$-th element is$(\\bb{R} _{\\mathpzcb{X}}) _{ij} = \\mathbb{E} \\mathpzc{X} _i \\mathpzc{X} _j$.Similarly, the covariance matrix is defined as the central counterpartof the above moment, its $(i,j)$-th element is$(\\bb{C} _{\\mathpzcb{X}}) _{ij} =\\mathrm{Cov}( \\mathpzc{X} _i , \\mathpzc{X} _j)$.Given another random vector$\\mathpzcb{Y} = (\\mathpzc{Y} _1, \\dots, \\mathpzc{Y} _m)$, thecross-correlation and cross-covariance matrices are defined as$\\bb{R} _{\\mathpzcb{X}\\mathpzcb{Y}} = \\mathbb{E}  \\mathpzcb{X} \\mathpzcb{Y}^\\Tr$and$\\bb{C} _{\\mathpzcb{X}\\mathpzcb{Y}} = \\mathbb{E}  (\\mathpzcb{X} - \\bb{\\mu} _\\mathpzcb{X} ) (\\mathpzcb{Y} - \\bb{\\mu} _\\mathpzcb{Y} )^\\Tr$,respectively. Linear transformations Let $\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ be an$n$-dimensional random vector, $\\bb{A}$ and $m \\times n$ deterministicmatrix, and $\\bb{b}$ and $m$-dimensional deterministic vector. We definea random vector $\\mathpzcb{Y} = \\bb{A} \\mathpzcb{X} + \\bb{b} $ as theaffine transformation of $\\mathpzcb{X}$. Using linearity of theexpectation operator, it is straightforward to show that Random signals A random signal or a stochastic process is a collection of randomvariables ${ \\mathpzc{F}(\\bb{x}) : \\bb{x} \\in D }$ indexed by some set$D$ (called the domain) and assuming values in some space $S$ (calledthe state space). For our purpose, we will assume $D$ to be either$\\mathbb{R}^d$ (in this case we will refer to the process as to acontinuous-domain stochastic process) or $\\mathbb{Z}^d$ (discrete-domainprocess); the state space $D$ will be assumed either $\\mathbb{R}$(continuous state) or $\\mathbb{Z}$ (discrete state). Informally, setting$D=\\mathbb{R}^2$ and $S=\\mathbb{R}$ we can think of $\\mathpzc{F}$ as ofa random image. When $d&gt;1$, it is customary to call the stochasticprocess a random field. We will henceforth use the term “randomsignal”. In what follows, we will almost always tacitly assume $D$ to be$\\mathbb{R}^d$; the very same reasoning applies to discrete-domainsignals mutatis mutandis. Formally, a random signal $\\mathpzc{F}$ is a function$\\mathpzc{F} : D \\times \\Omega \\rightarrow S$, where $\\Omega$ denotesthe sample space. The first argument $\\bb{x} \\in D$ in$\\mathpzc{F}(\\bb{x}, \\alpha)$ sets the location in the domain, while thesecond argument $\\alpha \\in \\Omega$ is responsible for the randomness.Fixing some $\\alpha \\in \\Omega$, the resulting deterministic function$f(\\bb{x}) = \\mathpzc{F}(\\bb{x}, \\alpha)$ is called a realization or asample function of $\\mathpzc{F}$. Fixing some $\\bb{x} \\in D$, weobtain a random variable$\\mathpzc{F}(\\alpha) =  \\mathpzc{F}(\\bb{x}, \\alpha)$ describing therandomness of the signal sampled at a fixed location $\\bb{x}$. Note thatfor a singletone $D = {1}$, the signal is just a random variable,while for a finite $D = {1,2,\\dots,n}$ it is a random vector. Randomsignals can be therefore thought of as a generalization of randomvectors to “random functions”. However, in such aninfinitely-dimensional case it is not easy to define the standardnotions such as density. Instead, we are going to sample the signal at aset of points ${ \\bb{x} _1,\\dots,\\bb{x} _n } \\subset D$ and describe thedistribution of the random vector$\\mathpzcb{F} = (\\mathpzc{F}(\\bb{x} _1),\\dots, \\mathpzc{F}(\\bb{x} _n))$.We will define the finite-dimensional CDF as Stationarity A random signal is said (strict sense) stationary (SSS), if itsprobability distribution is shift-invariant. In other words, if for any$n$, any ${ \\bb{x} _1,\\dots,\\bb{x} _n } \\subset D$, and any$\\bb{p} \\in D$,$(\\mathpzc{F}(\\bb{x} _1+\\bb{p}),\\dots, \\mathpzc{F}(\\bb{x} _n+\\bb{p}))$ hasthe same distribution as$(\\mathpzc{F}(\\bb{x} _1),\\dots, \\mathpzc{F}(\\bb{x} _n))$, then$\\mathpzc{F}$ is SSS. Auto-correlation and cross-correlation Moments of random signals can be defined by considering random vectorsobtained from sampling the signal at some set of locations. Ofparticular importance will be the following first- and second-ordermoments: the mean function and the auto-correlation function Given two random signals $\\mathpzc{F}$ and $\\mathpzc{G}$, we cansimilarly define their cross-correlation as It follows from definition that$R _{\\mathpzc{F}}(\\bb{x} _1,\\bb{x} _2)  = R _{\\mathpzc{F}}(\\bb{x} _2,\\bb{x} _1)$and$R _{\\mathpzc{G}\\mathpzc{F}}(\\bb{x} _1,\\bb{x} _2)  = R _{\\mathpzc{F}\\mathpzc{G}}(\\bb{x} _2,\\bb{x} _1)$.Two random signals are said to be orthogonal if their covariancefunction vanishes at every point. Wide-sense stationarity A random signal $\\mathpzc{F}$ is called wide-sense stationary (WSS) ifits mean and auto-correlation functions are shift-invariant, i.e., forevery $\\bb{p} \\in D$$\\mu _{\\mathpzc{F}}(\\bb{x} + \\bb{p}) = \\mu _{\\mathpzc{F}}(\\bb{x})$ forevery $\\bb{x} \\in D$, and$R _{\\mathpzc{F}}(\\bb{x} _1 + \\bb{p},\\bb{x} _2 + \\bb{p}) = R _{\\mathpzc{F}}(\\bb{x} _1,\\bb{x} _2)$for every $\\bb{x} _1,\\bb{x} _2 \\in D$. These conditions immediatelytranslate to demanding $\\mu _{\\mathpzc{F}} = \\mathrm{const}$ and$R _{\\mathpzc{F}}(\\bb{x} _1,\\bb{x} _2) = R _{\\mathpzc{F}}(\\bb{x} _1 - \\bb{x} _2)$.Two WSS random signals $\\mathpzc{F}$ and $\\mathpzc{G}$ are calledjointly WSS if their cross-correlation is shift-invariant, i.e.,$R _{\\mathpzc{F}\\mathpzc{G}}(\\bb{x} _1,\\bb{x} _2) = R _{\\mathpzc{F}\\mathpzc{G}}(\\bb{x} _1 - \\bb{x} _2)$. Power spectrum density We start our discussion with the more familiar domain of deterministicsignals. In the signal processing jargon, the energy of adeterministic signal $f$ is defined as Due to Parseval’s identity, (the same is true mutatis mutandis for discrete-domain signals). We cantherefore think of $| F(\\bb{\\xi})|^2$ as of the energy density of $f$per unit of frequency. When the signal has infinite energy, we can still define its averagepower by windowing the signal and normalizing its energy within thewindow by the volume of the latter, where $\\mathrm{rect} _T(\\bb{x}) = \\mathrm{rect}(\\bb{x}/T)$. Defining thewindowed Fourier transform and invoking Parseval’s identity, we have $\\lim _{T \\rightarrow \\infty} |F _T(\\bb{\\xi})|^2$ can be interpreted asthe density of power per unit of frequency and is often referred to asthe power spectral density (PSD). The same reasoning can be repeated for WSS random processes. While arandom process $\\mathpzc{F}$ has infinite energy, it generally hasfinite average power and one can define the PSD as Changing the integration variable $\\bb{x}’$ to$\\bb{y} = \\bb{x}-\\bb{x}’$, one obtains the following result known as theWiener-Khinchin theorem: This is a profound conclusion relating the PSD of a random signal to theFourier transform of its auto-correlation. Since $R _{\\mathpzc{F}}$ is an even function (i.e., invariant to spacemirroring), $S _{\\mathpzc{F}}$ is real-valued. However, it is also easyto show that $S _{\\mathpzc{F}} \\ge 0$. This stems from the fact that theauto-correlation is positive semi-definite and, in case of wide-sensestationarity, it is diagonalized by the Fourier transform; hence the itsspectrum is non-negative. Cross-spectral density The result can be generalized to a pair of jointly WSS random signals$\\mathpzc{F}$ and $\\mathpzc{G}$. We define the cross-spectal densityas The Wiener-Kinchin theorem states where the cross-correlation is defined as$R _{\\mathpzc{F}\\mathpzc{G}} (\\bb{x}) = \\mathbb{E} \\mathpzc{F}(\\bb{0}) \\mathpzc{G}(\\bb{x})$. LSI systems Let $\\mathpzc{F}$ be a WSS signal passing through a linearshift-invariant system $\\mathcal{H}$ with the impulse reponse $h$. Wedefine the output signal as $\\mathpzc{G} = h \\ast \\mathpzc{F}$.Straightforwardly, the mean function of $\\mathpzc{G}$ is given by where $H(\\bb{0})$ is usually called the DC response of $\\mathcal{H}$.Note that the mean function is constant. The auto-correlation function of $\\mathpzc{G}$ is given by where $ \\overline{h}(\\bb{x}) = h(-\\bb{x})$. Note that theauto-correlation function is shift-invariant (i.e., it does not dependon $\\bb{x}$). These two results imply that $\\mathpzc{Y}$ is WSS (notsurprising: a signal with shift-invariant moments passing through ashift-invariant system leads to a signal with shift-invariant moments). The input-output cross-correlation is given by Again note that the function is shift-invariant, implying that$\\mathpzc{F}$ and $\\mathpzc{G}$ are jointly WSS. Clearly, since$R _{\\mathcal{G}\\mathpzc{F}} = \\overline{R} _{\\mathcal{F}\\mathpzc{G}} = {h} \\ast \\overline{R} _\\mathpzc{F}  = {h} \\ast {R} _\\mathpzc{F}$. Translating the latter expressions to the frequency domain, we obtainthe following relations Compare this to linear transformations of random vectors. White and colored noise A random signal $\\mathpzc{N}$ with constant PSD,$S _{\\mathpzc{N}}(\\bb{\\xi}) = \\sigma _\\mathpzc{N}^2$ is usually calledwhite noise, by analogy with white light that has approximately flatspectrum3. Its auto-correlation is given by$R _{\\mathpzc{N}}(\\bb{x}) = \\sigma _\\mathpzc{N}^2 \\, \\delta$. When whitenoise passes through an LSI system $\\mathcal{H}$, the spectrum at theoutput,$S _{\\mathcal{H}\\mathpzc{N}}(\\bb{\\xi}) = |H(\\bb{\\xi})|^2 \\sigma _\\mathpzc{N}^2$,is shaped by the power response $|H(\\bb{\\xi})|^2$ of the system. Thisphenomenon is called as coloring. The auto-correlation function ofcolored noise is given by$R _{\\mathcal{H}\\mathpzc{N}} = \\sigma _\\mathpzc{N}^2 \\, h \\ast  \\delta \\ast \\overline{h} = \\sigma _\\mathpzc{N}^2 \\, h \\ast \\overline{h}$.             To be completely rigorous, the proper way to define the PDF is byfirst equipping the image of the map $\\mathpzc{X}$ with the Lebesguemeasure $\\lambda$ that assigns to every interval $[a,b]$ its length$b-a$. Then, we invoke the Radon-Nikodym theorem saying that if$\\mathpzc{X}$ is absolutely continuous w.r.t. $\\lambda$, thereexists a measurable function $f : \\mathbb{R} \\rightarrow [0,\\infty)$such that for every measurable $A \\subset \\mathbb{R}$,$\\displaystyle{(\\mathpzc{X} _\\ast P)(A) =P(\\mathpzc{X}^{-1}(A)) = \\int _A f d\\lambda}$.$f$ is called the Radon-Nikodym derivative and denoted by$\\displaystyle{f = \\frac{d(\\mathpzc{X} _\\ast P)}{d\\lambda}} $. It isexactly our PDF. &#8617;               In fact, $r _{\\mathpzc{X}\\mathpzc{Y}}$ can be viewed as an innerproduct on the space of random variables. This creates a geometryisomorphic to the standard Euclidean metric in $\\mathbb{R}^n$. Usingthis construction, the Cauchy-Schwarz inequality immediatelyfollows:$| r _{\\mathpzc{X}\\mathpzc{Y}} | \\le \\sigma _\\mathpzc{X} \\sigma _\\mathpzc{Y}$. &#8617;               This appears to be false when color perception is examined moreclosely! &#8617;       ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lecture_notes/lecture_4/",
        "teaser":null},{
        "title": "Lecture 5: Statistical estimation",
        "excerpt":"Inverse problems and estimation Let $\\mathpzc{F}$ be a latent WSS random signal of which a degradedobservation $\\mathpzc{Y} = \\mathcal{H} \\mathpzc{F} + \\mathpzc{N}$ isgiven, with $\\mathcal{H}$ being a known invertible LSI system (e.g.,out-of-focus or motion blur), and $\\mathpzc{N}$ is zero-mean additiveWSS white noise with $S_ \\mathpzc{N}  = \\sigma_ \\mathpzc{N}^2$statistically independent of $\\mathpzc{F}$ (we will get back to thequestion of how to model the noise more realistically). From ourprevious derivations, we straightforwardly have$S_ \\mathpzc{Y} = |H|^2 S_ \\mathpzc{F} + S_ \\mathpzc{N}$ and$S_ {\\mathpzc{F} \\mathpzc{Y}} = H^\\ast S_ \\mathpzc{F}$. Our goal is to invert the action of $\\mathcal{H}$ obtaining the latentsignal $\\mathpzc{F}$ from the observation $\\mathpzc{Y}$. Such a problemis known as (non-blind) deconvolution and falls into a more generalcategory of inverse problems. In the absence of noise($\\sigma_ \\mathpzc{N}=0$), $\\mathpzc{F}  = \\mathcal{H}^{-1} \\mathpzc{Y}$,where $\\mathcal{H}^{-1}$ is the inverse system with the frequencyresponse $1/H({\\bm{\\mathrm{\\xi}}})$. However, in practice this is veryoften a very bad idea. Even if $\\mathcal{H}$ is invertible in theory,its inverse might amplify even the slightest noise to unreasonableproportions. We will therefore phrase our problem as an estimationproblem: find an LSI system $\\mathcal{G}$ such that the estimator$\\hat{\\mathpzc{F}} = \\mathcal{G} \\mathpzc{Y}$ is optimally close to thetrue $\\mathpzc{F}$ in the sense of some error criterion. We define theerror signal (which is straightforwardly WSS), and seek a system $\\mathcal{G}$satisfying where $\\epsilon$ is some error criterion. In what follows, we discussseveral choices of $\\epsilon$ setting stage to a variety of estimationframeworks. We also discuss more general inverse problems. Wiener filter A very popular pragmatic choice is the mean squared error (MSE), This choise leads to minimum MSE (MMSE) estimators. Using the factthat we can write Substituting $ S_ \\hat{\\mathpzc{F}} = |G|^2  S_ {\\mathpzc{Y}}$ and$S_ {\\mathpzc{F} \\hat{\\mathpzc{F}}} = G^\\ast S_ {\\mathpzc{F} \\mathpzc{Y} }$yields In order to minimize $\\epsilon$ over $G({\\bm{\\mathrm{\\xi}}})$ it istherefore sufficient to minimize the above integrand for every${\\bm{\\mathrm{\\xi}}}$ individually; furthermore since the first term inthe integrand does not depend on $G$, we aim at minimizing (for every${\\bm{\\mathrm{\\xi}}}$, which is omitted for convenience) (Note that the first term does not depend on $G$). Observe that at frequencies where $S_ \\mathpzc{Y}$ vanishes, it can beshown that $S_ \\mathpzc{FY}$ vanishes as well (Cauchy-Schwarzinequality). Hence, at those frequencies we may arbitrarily set $G$ tozero. Otherwise, using the fact that $S_ \\mathpzc{Y}$ is real andnon-negative, we can write In the absence of other constraints (such as, e.g., bounded spatialsupport), the minimizer of the above expression is simply This result is known as the Wiener filter after the mathematicianNorbert Wiener. Note that since both $S_ {\\mathpzc{F}\\mathpzc{Y} }$ and$S_ \\mathpzc{Y}$ are Fourier transforms or real-valued functions, theyare conjugate symmetric, and so is their ratio. Consequently,$G_ \\ast({\\bm{\\mathrm{\\xi}}})$ is the frequency response of an LSI systemwith a real-valued impulse response. Estimation MSE Substituting $G_ \\ast$ into the MSE expression yields the error achievedby the Wiener filter, where plays the role of frequency-wise correlation coefficient. Orthogonality Observe that at every frequency, from where $R_ {\\hat{\\mathpzc{F}}\\mathpzc{Y} } = R_ {\\mathpzc{FY} }$.Hence, for every ${\\bm{\\mathrm{\\tau}}}$. This can be stated as the followingorthogonality property: $\\mathpzc{E} \\perp \\mathpzc{Y}$, that is, theestimation error is orthonogonal to the data. Orthogonality is thehallmark of $\\ell_ 2$-optimality, which is the case of MMSE estimators. Optimal deconvolution Note that the Wiener filter expression is general and does not assumeany specific relation between the latent signal $\\mathpzc{F}$ and theobservation $\\mathpzc{Y}$ except the assumption of joint wide-sensestationarity and that their cross-spectrum is known. In the specificcase of $\\mathpzc{Y} = \\mathcal{H} \\mathpzc{F} + \\mathpzc{N}$ withstatistically-independent additive zero-mean noise $\\mathpzc{N}$, wehave $R_ {\\mathpzc{Y}}({\\bm{\\mathrm{\\tau}}}) = S_ {(\\mathcal{H}\\mathpzc{F})}({\\bm{\\mathrm{\\tau}}}) + R_ {\\mathpzc{N}}({\\bm{\\mathrm{\\tau}}})$ and can write Similarly, Substituting into the Wiener filter expression we obtain afterelementary algebraic manipulations Let us define the signal-to-noise ratio (SNR) at frequency${\\bm{\\mathrm{\\xi}}}$ as Note that in the particular case of white noise with the spectrum$S_ \\mathpzc{N}  = \\sigma_ \\mathpzc{N}^2$, The filter expression can be written as At frequencies where $\\mathrm{SNR}({\\bm{\\mathrm{\\xi}}}) \\gg 1$ (signalis much stronger than the noise), which is exactly the inverse system. On the other hand, at frequencieswhere $\\mathrm{SNR}({\\bm{\\mathrm{\\xi}}})$ approaches zero, (noise muchstronger than the signal), the numerator becomes dominant and$G_ \\ast({\\bm{\\mathrm{\\xi}}})$ also approaches zero. Maximum likelihood estimators Kullback-Leibler divergence Let $P$ and $Q$ be two probability measures (such that $P$ is absolutelycontinuous w.r.t. $Q$). Then, the Kullback-Leibler divergence from Qto P is defined as In other words, it is the expectation of the logarithmic differencesbetween the probabilities $P$ and $Q$ when the expectation is taken over$P$. The divergence can be thought of as an (asymmetric) distancebetween the two distributions. Maximum likelihood Since $\\mathpzc{Y}= \\mathcal{H} \\mathpzc{F} + \\mathpzc{N}$, we canassert that the distribution of the measurement $\\mathpzc{Y}$ given thelatent signal $\\mathpzc{F}$ is simply the distribution of $\\mathpzc{N}$at $\\mathpzc{N} = \\mathpzc{Y}- \\mathcal{H} \\mathpzc{F}$, Assuming i.i.d. noise, the latter simplifies to a product ofone-dimensional measures. Note that this is essentially a parametricfamily of distributions – each choice of $f$ yields a distribution$P_ {\\mathpzc{Y} | \\mathpzc{F} = f}$ of $\\mathpzc{Y}$. For the timebeing, let us treat the notation $\\mathpzc{Y} | \\mathpzc{F}$ just as afunny way of writing. Given an estimate $\\hat{f}$ of the true realization $f$ of$\\mathpzc{F}$, we can measure its “quality” by measuring the distancefrom $P_ {\\mathpzc{Y} | \\mathpzc{F}=\\hat{f}}$ to the true distribution$P_ {\\mathpzc{Y} | \\mathpzc{F}=f}$ that created $\\mathpzc{Y}$, and try tominimize it. Our estimator of $f$ can therefore be written as where we used the Kullback-Leibler divergence to quantify the distancebetween the distributions. Note that we treat the quantity to beestimated as a deterministic parameter rather than a stochasticquantity. Let us have a closer look at the minimization objective Note that the first term (that can be recognized as the entropy of$\\log P_ {\\mathpzc{Y} | \\mathpzc{F}=f}$) does not depend on theminimization variable; hence, we have Let us now assume that $\\mathpzc{Y}$ is observed at some set of $N$spatial locations${ {\\bm{\\mathrm{x}}}_ 1, \\dots, {\\bm{\\mathrm{x}}}_ N }$; we will denote$\\mathpzc{Y}_ i = \\mathpzc{Y}({\\bm{\\mathrm{x}}}_ i)$. In this case, we canuse p.d.f.s to write This function is known as the negative log likelihood function. By thelaw of large numbers, when $N$ approaches infinity, Behold our minimization objective! To recapitulate, recall that we started with minimizing the discrepancybetween the latent parametric distribution that generated theobservation and that associated with our estimator. However, a closerlook at the objective revealed that it is the limit of the negative loglikelihood when the sample size goes to infinity. The minimization ofthe Kullback-Leibler divergence is equivalent to maximization of thelikelihood of the data coming from a specific parametric distribution, For this reason, the former estimator is called maximum likelihood(ML). ML deconvolution Back to our deconvolution problem. Assuming white Gaussian distributionof the noise with zero mean and variance $\\sigma^2_ {\\mathpzc{N}}$ yields this in turn gives rise to the following negative log likelihoodfunction In the limit case, we minimize which by Parseval’s identity is equal to The latter integral is obviously mimimized by $F = \\frac{Y}{H}$, whichwe know is a very bad idea in practice. Maximum a posteriori estimators Conditional distributions Before treating maximum a posteriori estimation, we need to brieflyintroduce the important notion of conditioning and conditionaldistributions. Recall our construction of a probability space comprisingthe triplet $\\Omega$ (the sample space), $\\Sigma$ (the Borel sigmaalgebra), and $P$ (the probability measure). Let $X$ be a randomvariable and $B \\subset \\Sigma$ a sub sigma-algebra of $\\Sigma$. We canthen define the conditional expectation of $\\mathpzc{X}$ given $B$ asa random variable $\\mathpzc{Z} = \\mathbb{E} \\mathpzc{X} | B$ satisfyingfor every $E \\in B$ (we are omitting some technical details such as, e.g., integrabilitythat $\\mathpzc{X}$ has to satisfy). Given another random variable $\\mathpzc{Y}$, we say that it generates asigma algebra $\\sigma(\\mathpzc{Y})$ as the set of pre-images of allBorel sets in $\\mathbb{R}$, We can then use the previous definition to define the conditionalexpectation of $\\mathpzc{X}$ given $\\mathpzc{Y}$ as Recall that expectation applied to indicator functions can be used todefine probability measures. In fact, for every $E \\in \\Sigma$, we mayconstruct the random variable $\\ind_ E$, leading to$P(E) = \\mathbb{E} \\ind_ E$. We now repeat the same, this time replacing$\\mathbb{E} $ with $\\mathbb{E} \\cdot | \\mathpzc{Y}$. For every$E \\in \\Sigma$, is a random variable that can be thought of as a transformation of therandom variable $\\mathpzc{Y}$ by the function $\\varphi$. We denote thisfunction as $P(E |\\mathpzc{Y})$ and refer to it as the (regular)conditional probability of $E$ given $\\mathpzc{Y}$. It is easy to showthat for every measurable set $B \\subset \\mathbb{R}$, Substituting $E = { \\mathpzc{X} \\in B}$ yields the conditionaldistribution of $X$ given $\\mathpzc{Y}$, It can be easily shown that $P_ {\\mathpzc{X} | \\mathpzc{Y}}$ is a validprobability measure on $\\Sigma$ and for every pair of measurable sets$A$ and $B$, If density exists, $P_ {\\mathpzc{X} | \\mathpzc{Y}}$ can be describedusing the conditional p.d.f. $f_ {\\mathpzc{X} | \\mathpzc{Y}}$ and thelatter identity can be rewritten in the form This essentially means that$f_ {\\mathpzc{XY} } (x, y) = f_ {\\mathpzc{X} | \\mathpzc{Y}} (x | y)  f_ \\mathpzc{Y}(y)$.Integrating w.r.t. $y$ yields the so-called total probability formula We can also immediately observe that if $\\mathpzc{X}$ and $\\mathpzc{Y}$are statistically independent, we have from where $f_ {\\mathpzc{X} | \\mathpzc{Y}} = f_ {\\mathpzc{X}}$. In thiscase, conditioning on $\\mathpzc{Y}$ does not change our knowledge of$\\mathpzc{X}$. Bayes’ theorem One of the most celebrate (and useful) results related to conditionaldistributions is the following theorem named after Thomas Bayes.Exchanging the roles of $\\mathpzc{X}$ and $\\mathpzc{Y}$, we have re-arranging the terms, we have in terms of probability measures, the equivalent form is Posterior probability maximization Recall that in maximum likelihood estimation we treated $\\mathpzc{F}$ asa deterministic parameter and tried to maximize the conditionalprobability $P(\\mathpzc{Y} | \\mathpzc{F})$. Let us now think of$\\mathpzc{F}$ as of a random signal and maximize its probability giventhe data, Invoking the Bayes theorem yields In the Bayesian jargon, $P_ {\\mathpzc{F}}$ is called the priorprobability, that is, our initial knowledge about $\\mathpzc{F}$ beforeany observation thereof was obtained; $P_ {\\mathpzc{F} | \\mathpzc{Y}}$ iscalled the posterior probability having accounted for the measurement$\\mathpzc{Y}$. Note that the term $P_ {\\mathpzc{Y} | \\mathpzc{F}}$ is ourgood old likelihood. Since we are maximizing the posterior probability,the former estimator is called maximum a posteriori (MAP). Taking negative logarithm, we obtain This yields the following expression for the MAP estimator The minimization objective looks very similar to what we had in the MLcase; the only difference is that now a prior term is added. In theabsence of a good prior, a uniform prior is typically assumed, whichreduces MAP estimation to ML estimation. MAP deconvolution Let us consider for examples our deconvolution problem. As a prior, weassume that $\\mathcal{D} \\mathpzc{F}$ is distributed normally i.i.d.with zero mean and variances $\\sigma^2_ \\mathrm{D}$, where $\\mathcal{D}$is a derivative of an appropriate order. This leads to the followingobjective function which by Parseval’s identity is equal to (Note the complex modulus – the integrands in the frequency domain arecomplex). The expression is minimized by minimizing at each frequency Note that this is a convex quadratic function in $F$ having a singleglobal minimum. Differentiating w.r.t. $F$ and demanding equality tozero yields, up to the factor of $2$, (refer to Chapter 4 in the Matrix Cook Book for the exact treatment ofderivatives w.r.t. a complex variable). The final expression for theFourier transform of the estimated signal is given by Note the resemblance to the Wiener filter: the ratio$ \\frac{\\sigma^2_ {\\mathpzc{N}} }{\\sigma^2_ {D}}$ controls the amount ofregularization applied to the inverse of $H$. When the noise is strong,the term $D^\\ast D$ dominates; for vanishing noise variance, theexpression reduces to $H^{-1}$. However, the MAP formalizm immediatelyallows incorporating more meaningful and realistic priors instead of ourtoy example. While not leading to closed-form solutions, they will stillform well-defined minimization problems that can be solved (or at leastattempted to be solved) iteratively. An important philosophical questionarises at this point: What is better – to use an incorrect model that weknow how to solve (e.g., the Wiener filter), or try to develop a bettermodel that we can hope to solve (e.g., some non-convex MAP estimationproblem)? Practice of the past two decades shows that the secondapproach may literally lead (and has, indeed, led) to revolutions.Without too much exaggeration, we can state that over $99\\%$ of recentstudies in image processing essentially revolve around finding a betterexpression for the prior probability $P_ {\\mathpzc{F} }$. Bayesian estimators Note that the posterior distribution $P_ {\\mathpzc{F} | \\mathpzc{Y}}$ canbe used to define conditional expectations. Let $\\ell( f, \\hat{f})$ besome loss function determining how far our estimate $\\hat{f}$ is fromthe true signal $f$. Then, we can define an error criterion and minimize it over estimators of the form $\\hat{f} = \\hat{f}(y)$.(Note that the expectation is evaluated w.r.t. the posteriordistribution $P_ {\\mathpzc{F} | \\mathpzc{Y}}$). This yields to theso-called Bayesian estimators The Wiener filter (MMSE estimator) is a particular case of Bayesianestimators where the loss is set to be Euclidean,$\\ell(f, \\hat{f}) = | f - \\hat{f} |^2_ {L^2(\\mathbb{R}^d}$, andestimators have the form $\\hat{f}(y) = h\\ast y$. For the particular choice of the loss function a $c$ approaches $0$, the Bayesian estimator estimates the mode of theposterior distribution and thus approaches the MAP estimator, providedthat the distribution of $\\mathpzc{F}$ is unimodal. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lecture_notes/lecture_5/",
        "teaser":null},{
        "title": "Lecture 6: Patch-based priors",
        "excerpt":"We continue our quest for a better model for the prior probability$P_ {\\mathpzc{F}}$ to be used in MAP and Bayesian estimators. While it isalmost hopeless to be able to formulate such a prior on the entireimage, it is a much more practical task to do it on a small region of animage (known in the image processing jargon as a patch). Let us firstfix some size $T$ and define a square domain$\\square = \\left[-\\frac{T}{2},\\frac{T}{2}\\right]^d$. A patch around somepoint ${\\bm{\\mathrm{x}}}$ in an image $f$ can be thought of as$\\tau_ {\\bm{\\mathrm{x}}} f |_ \\square$. We can therefore define a patchoperator$\\pi_ {\\bm{\\mathrm{p}}} : \\mathbb{F}( {\\bm{\\mathrm{R}}}^d, {\\bm{\\mathrm{R}}}) \\rightarrow \\mathbb{F}( \\square, {\\bm{\\mathrm{R}}})$taking $f$ as the input and producing$\\tau_ {\\bm{\\mathrm{x}}} f |_ \\square$ as the output. In these terms, weredefine the problem of modeling the prior probability of a naturalimage $\\mathpzc{F}$ as the problem of modeling the prior probability of$\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{F}$ for every location${\\bm{\\mathrm{x}}}$ in the image domain. To that end, let us assume tohave access to a (potentially, very large) collection of clean patches${p_ 1,\\dots,p_ K}$; such a collection can be obtained by taking a verylarge representative set of images, decomposing them into a collectionof overlapping patches and clustering the latter into $K$ samples. Wewill further assume that a patch in a natural image$\\pi_ {\\bm{\\mathrm{q}}} \\mathpzc{F}$ is uniformly distributed over thatcollection, i.e., $\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{F} = p_ k$ withprobability $\\frac{1}{K}$ for $k=1,\\dots,K$. Note that the simpleassumption of uniform distribution over the collection of examplepatches leads to a potentially very intricate prior distribution of$\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{F}$ itself, since the collectioncaptures the intricate structure of representative image patches! Patch-based denoising Denoising using multiple images Let us simplify our archetypal inverse problem assuming that the latentsignal $\\mathpzc{F}$ is only degraded by white additive noise,$\\mathpzc{Y} = \\mathpzc{F} + \\mathpzc{N}$, admitting a Gaussiandistribution$\\mathpzc{N}({\\bm{\\mathrm{x}}}) \\sim \\mathcal{N}(0,\\sigma_ \\mathpzc{N}^2)$.This problem is known as Gaussian denoising. The likelihood of$\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{Y}$ given that$\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{F}$ was formed using the patch $p_ k$is simply given by the density of $\\pi_ \\bm{\\mathrm{x}} \\mathpzc{N}$, where the norm is the $L^2$ norm on $\\square$. Since we asserted uniformdistribution of $k$ over ${1,\\dots,K}$, the Bayes formula readliyyields Using this posterior, we can define the MSE of the estimator $\\hat{f}$at point ${\\bm{\\mathrm{x}}}$ as the MMSE estimator is therefore defined as where$h_ {k}( {\\bm{\\mathrm{x}}}  ) =  P_ {\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{F} |\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{Y}   } ( \\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{F} = p_ k | \\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{Y}  =  \\pi_ {\\bm{\\mathrm{x}}} y )$.Note that the sum of the latter weights over $k$ is always one. Alsonote that $ p_ k({\\bm{\\mathrm{0}}})$ gives the value at the center of thepatch. The latter problem has a simple closed-form solution as theweighted average of the central values from exemplar patches ${p_ 1,\\dots,p_ K}$, withweights inversely proportional to the distance of the said patches fromthe corresponding input patch centered at ${\\bm{\\mathrm{x}}}$. Since $K$might be very big and the exponential anyway decays very fast in$ | \\pi_ {\\bm{\\mathrm{x}}} y - p_ k  |$, in practice the above sum isapproximated by taking a fixed number of (approximate) nearestneighbors) in the sense of the inter-patch distance$ | \\pi_ {\\bm{\\mathrm{x}}} y - p_ k  |$. Non-local means It is sometimes inconvenient to assume the availability of exemplarpatches ${p_ k}$ coming from an external collection of images; instead,it is often desirable to use the patches from the image itself for thepurpose of defining the prior on $\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{F}$.The former MMSE estimator can be readily formulated in this setup as or, more practically, where${ \\pi_ {\\bm{\\mathrm{x}}_ 1} y, \\dots, \\pi_ {\\bm{\\mathrm{x}}_ K} y }$are the $K$ nearest neighbors of $\\pi_ {\\bm{\\mathrm{x}}} y$ in thenoisy image itself $y$ (variants of the PatchMatch are very useful tocompute the nearest neighbors). Such a filter is known as non-localmeans (NLM) – it works like a regular local averaging that supressesnoise (by the law of large numbers), yet, in the case of NLM averagingis non-local. Oftentimes, an additional (very slowly decaying) spatialweight is added to down-weigh spatially distant pixels, where $\\sigma^2_ \\mathrm{s}$ controls the width of the spatial weight. Bilateral filter A particularly renowned setting of NLM (that was actually devised priorto NLM) is the case of point-wise patches, i.e.,$\\pi_ {\\bm{\\mathrm{x}}} : f \\rightarrow f({\\bm{\\mathrm{x}}})$. In thiscase, the NLM estimator simplifies to which is known as the bilateral filter. Bilateral filter can bethought of as a non-shift invariant linear filter, with the spatially-varying unit-DC impulse response Another useful model is to consider the so-called lifted space${\\bm{\\mathrm{R}}}^d \\times {\\bm{\\mathrm{R}}}$, in which the graph ofthe image $f$ resides. The graph can be modeled as a delta distributionon ${\\bm{\\mathrm{R}}}^d \\times {\\bm{\\mathrm{R}}}$ supported on$({\\bm{\\mathrm{x}}},f({\\bm{\\mathrm{x}}})$ (note that every function $f$has such a representation, but not every function or distribution on${\\bm{\\mathrm{R}}}^d \\times {\\bm{\\mathrm{R}}}$ represents a validfunction). Then, convolving the latter distribution with an LSI Gaussiankernel, and marginalizing over $f$ yields precisely the bilateral filter. NLM as MMSE Note that in its original formulation, the NLM estimator is not an MMSEestimator. This stems from the fact that when using the patches of theimage itself as exemplars to define the prior on$\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{F}$, the patches are contaminated withnoise! Fixing a set of $K$ locations ${\\bm{\\mathrm{x}}}_ k$ from whichexemplar patches are taken, we now do not have access to $p_ k$ butrather to their version contaminated by noise,$q_ k = p_ k + \\pi_ {\\bm{\\mathrm{x}}_ k} \\mathpzc{N} = p_ k + \\mathpzc{N}_ k$. Let us repeat the derivation of the posterior probability distribution.The likelihood of $\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{Y}$ given that$\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{F}$ was formed using the patch $p_ k$is simply given by the density of $\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{N}$, however, we no more have access to $p_ k$. Instead, we can write By definition of random Gaussian noise, since the norm is computed on$\\square$ of volume $T^d$,$\\mathbb{E} | \\mathpzc{N}_ k |^2 = T^d \\sigma^2_ {\\mathpzc{N}}$. Letting$y = p_ k + \\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{N}$, the third term can bewritten as Even though the denoised and the reference patches may overlap, we willassume that $ \\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{N}$ and $\\mathpzc{N}_ k$are statistically independent and, since they are zero mean, they areorthogonal. This yields$\\mathbb{E} \\langle y-q_ k,  \\mathpzc{N}_ k \\rangle = -T^d \\sigma^2_ {\\mathpzc{N}}$;combining the previous results, we have Similarly to the noiseless exemplars case, the Bayes formula yields Using this posterior, we again define the MSE of the estimator $\\hat{f}$at point ${\\bm{\\mathrm{x}}}$ as In the previous case, this yielded an estimator in the form of theweighted sum with $c_ k = h_ k({\\bm{\\mathrm{x}}})$ summing to $1$. However, note that $p_ k$are now unaccessible, so we have to write with some other set of weights ${\\bm{\\mathrm{c}}}$ also summing to $1$.This yields where ${\\bm{\\mathrm{P}}}$ is a $K \\times K$ matrix with the elements$({\\bm{\\mathrm{P}}})_ {ij} = \\langle p_ i, p_ j \\rangle$,${\\bm{\\mathrm{h}}}$ is the $K$-dimensional vector with the elements$h_ k({\\bm{\\mathrm{x}}})$, and ${\\bm{\\mathrm{w}}}$ is the $K$-dimensionalvector with the elements $({\\bm{\\mathrm{w}}})_ i = |  p_ i |^2$. OurMMSE estimator is therefore given as the solution to the constrainedminimization problem Defining the Lagrangian$ \\epsilon( {\\bm{\\mathrm{c}}} ) + \\lambda {\\bm{\\mathrm{1}}}^\\Tr {\\bm{\\mathrm{c}}}$,differentiating w.r.t. ${\\bm{\\mathrm{c}}}$ and setting the gradient tozero yields with the constant satisfying the unit sum constraint. Note the inversion of the potentially very big matrix${\\bm{\\mathrm{P}}} + T^d  \\sigma_ \\mathpzc{N}^2 {\\bm{\\mathrm{I}}}$ makesthis solution impractical. However, assuming that for every $i$,$\\langle p_ i, p_ i \\rangle = T^d s^2_ {\\mathpzc{F}}$ and for every$j \\ne i$, $\\langle p_ i, p_ j \\rangle = \\rho T^d s^2_ {\\mathpzc{F}}$,where$s^2_ {\\mathpzc{F}} = {\\bm{\\mathrm{E}}} \\mathpzc{F}({\\bm{\\mathrm{x}}})$,the matrix the matrix ${\\bm{\\mathrm{P}}}$ assumes the simple form${\\bm{\\mathrm{P}}} = \\alpha {\\bm{\\mathrm{I}}} + \\beta {\\bm{\\mathrm{1}}}{\\bm{\\mathrm{1}}}^\\Tr$,where $\\alpha =   T^d (1-\\rho)s^2_ {\\mathpzc{F}}$ and$\\beta = T^d \\rho  s^2_ {\\mathpzc{F}}$. The matrix to invert thereforebecomes where $\\gamma =  \\alpha +  T^d \\sigma_ \\mathpzc{N}^2$. Using theSherman-Morrison matrix identity, the inverse can be expressed as Substituting the latter result to the formula for${\\bm{\\mathrm{c}}}_ \\ast$, after tedious and boring algebra, we arrive at Note that the weight vector is given by a weighted average (a convexcombination) of the posterior probabilities ${\\bm{\\mathrm{h}}}$ and theuniform vector $\\frac{1}{K} \\, {\\bm{\\mathrm{1}}}$. Interpreting thesignal to noise ratio as we can rewrite For high SNR ($\\sigma^2_ {\\mathpzc{N}}$ approaching $0$), we obtain theclassical NLM with $c_ k = h_ k$. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lecture_notes/lecture_6/",
        "teaser":null},{
        "title": "Lecture 7: Sparsity-based priors",
        "excerpt":"We continue our quest for a better model for the patch prior probability$P_ {\\mathpzc{F}} (\\pi_ {\\bm{\\mathrm{x}}} \\mathpzc{F}) $ to be used inMAP and Bayesian estimators. This time we consider the family of priorsbased on the assertion that a patch admits a sparse representation insome dictionary. Kurtosis and sparsity Let us start with a little background in probability theory. Let usconsider the Gaussian distribution of a random variable given by thedensity function and fully characterized by the mean $\\mu$ and variance $\\sigma^2$. Since thedensity function has a negative exponential of the square of $x$, itstails decay very fast. However, there might be other distributions withmuch “heavier” tails, that is, containing more probability in the tails. A common measure for the “tailedness” of a distribution is the notion ofkurtosis, defined as the normalized central fourth moment, All Gaussian distributions happen to have kurtosis $3$. It is customaryto define the kurtosis excess as $\\mathrm{Kurt}\\,\\mathpzc{X} - 3$.Distributions with positive kurtosis excess are called super-Gaussianor leptokurtic or, colloquially, as “heavy-tailed”. Positive curtosisexcess arises in two circumstances: when the probability mass isconcentrated around the mean and the data-generating process producesoccasional values far from the mean, and when the probability mass isconcentrated in the tails of the distribution. On the contrary, when thekurtosis excess is negative, the distribution is called sub-Gaussianor platykurtic or, informally, as “light-tailed”. The exponential power distribution given by the density function can be thought of a generalized Gaussian distribution with the mean andvariance controlled by the parameters $\\mu$ and $\\alpha$, respectively,and the shape of the distribution controlled by the power $p$. Note that$p=2$ yields exactly the Gaussian distribution with$\\mathrm{Kurt}\\,\\mathpzc{X} - 3 = 0$. For $p&lt;2$, a super-Gaussian familyof distributions is obtained with the notable case of the Laplace(a.k.a. double-exponential) distribution corresponding to $p=1$. For$p&gt;2$, the family is sub-Gaussian converging pointwise to the uniformdistribution on $[\\mu-\\alpha,\\mu+\\alpha]$ as $p \\rightarrow \\infty$. Let ${\\bm{\\mathrm{x}}}$ be an $n$-dimensional vector sampled i.i.d. froma zero-mean super-Gaussian random variable. Since the variable willoften realize values around zero, and occasionally large non-zerovalues, most of the elements of the vector will be nearly zero, withsome elements (at uniformly random indices) having a large magnitude.This can be thought of as a probabilistic formalization of the statement“vector ${\\bm{\\mathrm{x}}}$ is sparse”. In this sense,super-Gaussianity leads to sparsity. Bases, frames and dictionaries We will now need a few important notions in linear algebra andfunctional analysis. Recall that we defined a patch of a signal $f$centered at ${\\bm{\\mathrm{x}}}$ as the function $f$ translated by${\\bm{\\mathrm{x}}}$ and restricted to the domain$\\square = \\left[-\\frac{T}{2},\\frac{T}{2}\\right]^d$. For notationconvenience, in the following treatment we refer to the patch by thename $f$ itself. Recall that any continuous function on $\\square$ couldbe decomposed into the Fourier series with and the coefficients$c_ {\\bm{\\mathrm{n}}} = \\langle f, \\phi_ {\\bm{\\mathrm{n}}} \\rangle_ {L^2(\\square)}$.An important observation here is that the functions${ \\phi_  {\\bm{\\mathrm{n}}} }_  {\\bm{\\mathrm{n}} \\in \\mathbb{Z}^d}$are linearly-independent and span the space of functions$L^2(\\square)$. Such a set is called a basis of the said space. Thecrucial feature of a basis is the uniqueness of the decomposition. Inother words, there exists only one set of coefficients${  c_ {\\bm{\\mathrm{n}}} }$ representing $f$ in${ \\phi_ {\\bm{\\mathrm{n}}} }_ {\\bm{\\mathrm{n}}}$; in the particularcase discussed above, the basis is furthermore orthonormal, so we have asimple formula for computing the coefficients using orthogonalprojections on the basis functions1. Let us now be given a collection of linearly-dependent functions${ \\phi_ {\\bm{\\mathrm{n}}} }_ {\\bm{\\mathrm{n}}}$ spanning$L^2(\\square)$. Such vectors no more form a basis and therefore thebenefit of unique representation is lost. However, as we will see in thesequel, the loss of uniqueness can be actually a very powerful tool. Aformal generalization of a basis is the notion of a frame definedthrough the following condition: if there exists two constants$0 &lt; A \\le  B &lt; \\infty$ such that for every $f$ in the space the set ${ \\phi_ {\\bm{\\mathrm{n}}} }_ {\\bm{\\mathrm{n}}}$ is called aframe. A proper frame (that is, which is not a basis) is calledovercomplete or redundant. Intuitively, it contains “more” functionsthat a basis would need. Formally, there exist an infinite set ofcoefficients $c = { c_ {\\bm{\\mathrm{n}}}  }$ representing $f$ w.r.t.${ \\phi_ {\\bm{\\mathrm{n}}} }$. We will associate with the frame the synthesis operator$\\Phi : \\ell^2 \\rightarrow  L^2(\\square)$ producing from the sequence of coefficients ${ c_ {\\bm{\\mathrm{n}}} }$. Theadjoint analysis operator$\\Phi^\\ast : L^2(\\square) \\rightarrow \\ell^2$ mapps $f$ to $c$. In the signal processing jargon, it is customary to speak of a frame asof a dictionary and of the constituent functions as of atoms. Then,a function $f$ can be represented as a superposition of atoms. If thedictionary is overcomplete, such a representation is not unique. For example, the set of functions with $a&gt;1$ forms a frame on $\\square$. We can think of it as anovercomplete Fourier transform dictionary. Taking the real value yieldsthe overcomplete cosine dictionary, There exists a plethora of other useful dictionaries such as wavelets,ridgelets, curvelets, countourlets (which all have a beautiful theory)and there also exists a possibility to learn the dictionary (optimalin some sense) from examples. The latter approach has been shownadvantageous in many applications. Discrete dictionaries When the patch domain is sampled, say, on the lattice$\\frac{T}{N} \\mathbb{Z}^d$, $f$ is given on a finite set of $(2N+1)^d$points with ${\\bm{\\mathrm{n}}} \\in { -N, \\dots, N}^d$. Reordering the samples$f[{\\bm{\\mathrm{n}}}]$ into a long $n=(2N+1)^d$-dimensional vector${\\bm{\\mathrm{f}}}$, we can think of the dictionary representation as where ${\\bm{\\mathrm{\\Phi}}}$ is an $n \\times k$ matrix whose columns are thesampled versions of the frame functions $\\phi_ {\\bm{\\mathrm{k}}}$sampled and reodered into $n$-dimensional vectors,${\\bm{\\mathrm{\\phi}}}_ {\\bm{\\mathrm{k}}}$ that are ordered in someorder such that there are $k = an &gt; n$ functions. The overcompletecosine frame readily becomes the overcomplete discrete cosinetransform (DCT) dictionary. Sparse patch priors A crucial empirical observation is that patches of natural images can beapproximated by a small number of atoms in many overcompletedictionaries such as the overcomplete cosine dictionary. We willformalize this by letting and asserting that the coefficients $c_ {\\bm{\\mathrm{n}}}$ are i.i.d.with a super-Gaussian distribution, while the residual $\\mathpzc{E}$ iswhite Gaussian with the variance $\\sigma^2_ \\mathpzc{E}$. This leads to the following negative log likelihood: with the negative log prior density of the coefficients, For example, using the exponential power family for $f_ c$ leads to Invoking the Bayes theorem leads to the following posterior density Note that in order to promote sparsity of the coefficients $p&lt;2$ has tobe small. However, for $p&lt;1$, the negative log density is non-convex,which is a major complication as we eventually would like to add it as aprior term to our optimization problem in a Bayesian or MAP estimatior.A pragmatic choice is $p=1$, the smallest value of $p$ keeping the termconvex (yet making it non-smooth at zero). This choice corresponds tothe Laplacian distribution, leading to the following aggregate of the$L_ 2$ norm with the $\\ell_ 1$ norm: where $\\lambda = \\frac{\\alpha}{\\sigma^2_ \\mathpzc{E}}$. Some flavors of sparse priors use dictionaries with non-negativefunctions and further assert non-negative coefficients,$c_ {\\bm{\\mathrm{n}}}({\\bm{\\mathrm{x}}}) \\ge 0$. This incorporates theprior information that the signal is non-negative, and is often employedfor modelling images and spectral magnitudes of audio signals. MAP estimation with a sparse patch prior At this point, we have two possibilities. We can rephrase our model as estimate the contents of a patch directly as where This can be done for every ${\\bm{\\mathrm{x}}}$; afterwards, theoverlapping estimated patches are aggregated by simple averaging.However, such an avergaing steers us farther away from the assumption ofsparse representation, as a sum of sparsely represented functions isless sparsely represented. A cure can be a smarter way to aggregate thepatches. An alternative approach is to estimate the entire signal $f$ by solving simultaneously for $f$ and${ c_ {\\bm{\\mathrm{n}}}({\\bm{\\mathrm{x}}}) }$ at all patches. Iterative shrinkage Regardless of which flavor of the above MAP estimators we choose, bothrequire the minimization of an aggregate of the $L_ 2$ and $\\ell_ 1$norms. We will rewrite this problem in the form where the inner products are on $L^2(\\square)$ and the $\\ell_ 1$ norm ison the space of sequences. This problem bears the name of Lasso (shortfor least absolute shrinkage and selection operator) in statistics. Note that the objective is a function of the sequence $c$ with the firsttwo terms differentiable that we will denote as and a non-differentiable third term,  Letus fix some $c$ and approximate $g(c)$ around $c$ as Here $\\eta$ controls the curvature of the second-order term in theapproximation. Plugging this approximation into the minimization problemyields where Note that the latter problem is coordinate-separable, so we can considerthe following one-dimensional minimization problem: Since the problem is non-smooth at $u=0$, we cannot readily take a derivativew.r.t. to $u$ and compare it to zero; instead, we have to use thesub-differential set This yields $u = y - \\lambda \\alpha$ for $\\alpha \\in \\partial |u|$.Whenever $z \\in [-\\lambda, \\lambda]$, we can set $u=0$ with$\\alpha = \\frac{z}{\\lambda} \\in  \\left( \\partial |u| \\right|_ {u=0}$.Otherwise, $u \\ne 0$ and we have $\\alpha = \\mathrm{sign}\\, u$ yielding$u = z - \\lambda \\, \\mathrm{sign}\\, u$. If $z&gt;\\lambda$, the right handside is positive, hence $u&gt;0$ and $\\mathrm{sign}\\, u = 1$; this yields$u = z-\\lambda$. Similarly, for $y &lt; -\\lambda$, we have $u&lt;0$ and hence$u = z + \\lambda$. We can therefore summarize the solution to theproblem as This operation on $z$ is calledsoft thresholding or shrinkage2 and will be denoted by$u = \\mathcal{S}_ {\\lambda}(z)$. Plugging this result back into our original multi-dimensional problemyields where in the last passage the shrinkage operator$ \\mathcal{S}_ { \\eta \\lambda }$ is applied element-wise to the sequence.We can repeat the process iterartively starting at some initial $c^0$(upperscript indices denote iteration number), yielding a process known as iterative shrinkage (a.k.a. iterativeshrinkage and thresholding algorithm or ISTA for short)3. Note thatthe step $c^{k} - \\eta \\, \\Phi^\\ast (\\Phi c^{k} - y)$ inside theshrinkage operator is a gradient descent step at point $c^k$ with thestep size $\\eta$, and it would be exactly the step if the objectivelacked the term $h(c)$. The shrinkage operator accounts for thisadditional term. Non-negativity In the case of non-negative dictionaries with non-negative coefficients,we can modify our one-dimensional Lasso problem by adding anon-negativity constraint on $u$, The solution is obtained in the same manner, except thart now $u$ cannotattain negative values. This leads to the one-sided shrinkage operator This operator is known under the name of rectified linear unit (or ReLU forshort) in the deep learning literature. We will explore this surprisingconnection more in the sequel.             In the general case, projection onto the bi-orthonormal set offunctions yields the coefficients. &#8617;               This operator can be viewed as the proximity map of the function$\\lambda |c|$. &#8617;               This is a member of a larger family of non-smooth optimizationalgorithms known as proximal methods. &#8617;       ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lecture_notes/lecture_7/",
        "teaser":null},{
        "title": "Lecture 1: Introduction",
        "excerpt":"                  Slides Part I: Introduction         Videos Introduction   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lectures/lecture_1/",
        "teaser":"https://vistalab-technion.github.io/cs236860/semesters/w1819/assets/images/lec1/imaging-eye.jpg"},{
        "title": "Lecture 2: Multidimensional signals & systems",
        "excerpt":"                  Slides Multidimensional signals and systems         Videos Multidimensional signals and systems   Lecture Notes Accompanying notes for this lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lectures/lecture_2/",
        "teaser":"https://vistalab-technion.github.io/cs236860/semesters/w1819/assets/images/lec2/lec2.png"},{
        "title": "Lecture 3: Sampling and interpolation",
        "excerpt":"                  Slides Sampling and interpolation         Videos Sampling and Interpolation   Lecture Notes Accompanying notes for this part of the lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lectures/lecture_3/",
        "teaser":"https://vistalab-technion.github.io/cs236860/semesters/w1819/assets/images/lec3/sampling.png"},{
        "title": "Lecture 4: Discrete-domain signals and systems",
        "excerpt":"                  Slides Discrete-domain signals and systems         Video   Lecture Notes Discrete-domain signals and systems Accompanying notes for this part of the lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lectures/lecture_4/",
        "teaser":"https://vistalab-technion.github.io/cs236860/semesters/w1819/assets/images/lec4/lecture-4-teaser.png"},{
        "title": "Lecture 5: Inverse problems & statistical estimation",
        "excerpt":"                  Slides         Videos Part I   Part II   Lecture Notes Inverse problems and statistical estimation Accompanying notes for this part of the lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lectures/lecture_5/",
        "teaser":"https://vistalab-technion.github.io/cs236860/semesters/w1819/assets/images/lec5/lecture-5-teaser.png"},{
        "title": "Lecture 6: Patch-based priors",
        "excerpt":"                  Slides Patch-based priors         Video   Lecture Notes Patch-based priors Accompanying notes for this part of the lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lectures/lecture_6/",
        "teaser":"https://vistalab-technion.github.io/cs236860/semesters/w1819/assets/images/lec6/lec6.png"},{
        "title": "Lecture 7: Sparsity-based priors",
        "excerpt":"                  Slides Sparsity-based priors         Video   Lecture Notes Accompanying notes for this part of the lecture can be found here. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lectures/lecture_7/",
        "teaser":"https://vistalab-technion.github.io/cs236860/semesters/w1819/assets/images/lec7/lec7.png"},{
        "title": "Lecture 8: Structured Sparsity",
        "excerpt":"                  Slides Structured Sparsity         Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lectures/lecture_8/",
        "teaser":"https://vistalab-technion.github.io/cs236860/semesters/w1819/assets/images/lec8/lec8.png"},{
        "title": "Lecture 9: Learning image priors",
        "excerpt":"                  Slides Learning image priors         Guest lecture: Regularization by Denoising, Prof. Michael Elad         Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lectures/lecture_9/",
        "teaser":"https://vistalab-technion.github.io/cs236860/semesters/w1819/assets/images/lec9/lec9-1.png"},{
        "title": "Lecture 10: Rethinking sensing & sampling",
        "excerpt":"                  Slides Rethinking sensing &amp; sampling         Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lectures/lecture__10/",
        "teaser":"https://vistalab-technion.github.io/cs236860/semesters/w1819/assets/images/lec10/lec10.png"},{
        "title": "Lecture 11: Computational imaging",
        "excerpt":"                  Slides Computational imaging         Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/lectures/lecture__11/",
        "teaser":"https://vistalab-technion.github.io/cs236860/semesters/w1819/assets/images/lec11/lec11-1.png"},{
        "title": "Welcome to CS236860!",
        "excerpt":"The course will take place on Mondays. Lecture: 08:30-10:30. Tutorial: 13:30-14:30. Please note that due to the faculty renovations, the lectures and the tutorials will be held inUlman 102. -Alex &amp; Sanketh ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/10/09/Welcome/",
        "teaser":null},{
        "title": "Lecture 1 material",
        "excerpt":"The slides presented during lecture 1 can now be found on the course website inthe lectures section. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/10/23/Lecture1/",
        "teaser":null},{
        "title": "Tutorial 1 material",
        "excerpt":"The material for tutorial 1 can now be found on the course website in thetutorials section. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/10/25/Tutorial_1/",
        "teaser":null},{
        "title": "New material added",
        "excerpt":"Dear students,   A new supplementon probability and statistics is now available in the supplementary material.Enjoy! The course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/10/26/Lecture2/",
        "teaser":null},{
        "title": "Lecture & Tutorial - 2",
        "excerpt":"Dear students, The lecture notes, slides, and the tutorial have been uploaded. Enjoy! The course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/10/30/Lec_Tut_2/",
        "teaser":null},{
        "title": "Lecture Slides & Tutorial - 3",
        "excerpt":"Dear students, The lecture slides, and the tutorial of the third week have been uploaded. Lecture notes for the second half of the lecture will be uploaded soon. Enjoy! The course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/11/06/Lec-Tut-3/",
        "teaser":null},{
        "title": "Video lectures",
        "excerpt":"Dear students, The video lecutures are available in this playlist. This week’s lecture slides, lecture notes, and the tutorial will follow in a day or so. Enjoy! The course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/11/12/Video-Lectures/",
        "teaser":null},{
        "title": "Lecture 4, 5 notes",
        "excerpt":"Dear students, Please find the lecture notes on discrete-domain signals and systems (Lecture 3 &amp; 4) and random signals (Lecture 5) uploaded here. We recommend to have a short overlook on the note on random signals and the supplementary note on probablity and statistics prior to the next class for enhanced understanding. Enjoy! The course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/11/14/lec_notes_4_5/",
        "teaser":null},{
        "title": "Video lecture & tutorial 4",
        "excerpt":"Dear students, The lecture video on discrete-domain signals and systems and the tutorial 4 are added. The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/11/15/tut4-lecvid4/",
        "teaser":null},{
        "title": "Next lecture",
        "excerpt":"Dear students, We recommend to have a short overlook on the note on random signals and the supplementary note on probablity and statistics prior to the next class for enhanced understanding. The course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/11/17/next-class/",
        "teaser":null},{
        "title": "Homework 1",
        "excerpt":"Dear students, The home assignment 1 is uploaded here. A dedicated page for the homeworks would be created soon, which will contain the submission details. Enjoy! The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/11/19/HW1/",
        "teaser":null},{
        "title": "Project list",
        "excerpt":"Dear students, Here is the link to list of papers for the project. Please comment your choices. PS: Please be careful to not resolve others’ comments. Enjoy! The course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/11/19/projects/",
        "teaser":null},{
        "title": "Lecture, Tutorial - 5",
        "excerpt":"Dear students, The lecture (slides and video) and tutorial 5 are online. Enjoy! The course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/11/20/lectut5/",
        "teaser":null},{
        "title": "Lecture, tutorial - 6",
        "excerpt":"Dear students, The lecture notes, videos, slides and the tutorial of lecture 6 are online. The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/12/09/lecture6-7/",
        "teaser":null},{
        "title": "Lecture 7 - slides, notes",
        "excerpt":"Dear students, The lecture notes and slides of lecture 7 (on patch-based priors) are now online. Notes: here.Lecture: here It is named “Lecture 6” in the lectures section for a content-wise organization of lectures. The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/12/09/lecture7/",
        "teaser":null},{
        "title": "Tutorial-7 canceled",
        "excerpt":"Dear students, The tutorial on December 10th (Tutorial 7) is canceled. It will be compensated in the near future. The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/12/09/tutorialcanceled/",
        "teaser":null},{
        "title": "HW1 - Submission",
        "excerpt":"Dear students, Please submit your homeworks to sanketh@cs.technion.ac.il The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/12/17/hw1-update/",
        "teaser":null},{
        "title": "Lecture-7 slides",
        "excerpt":"Dear students, Lecture 7 (Sparsity-based priors) slides are now online. The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/12/17/lec7slides/",
        "teaser":null},{
        "title": "Lecture-8 slides",
        "excerpt":"Dear students, Lecture 8 (Structured sparsity) slides are now online. The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/12/24/lec8slides/",
        "teaser":null},{
        "title": "Lecture-9 slides, Tutorials",
        "excerpt":"Dear students, Lecture 9 (Learning image models) slides are now online. The tutorials are up-to-date as well. The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2018/12/30/lec9slides/",
        "teaser":null},{
        "title": "Lecture-8 video, Tutorial-9",
        "excerpt":"Dear students, The video of Lecture 8 (Structured sparsity), and the Tutorial 9 (ISTA, BM3D) are online. The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2019/01/05/lec8video-tut9/",
        "teaser":null},{
        "title": "Homework 2",
        "excerpt":"Dear students, The home assignment 2 is uploaded here. The submission deadline is 6th February 2018 11:55PM Enjoy! The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2019/01/06/hw2/",
        "teaser":null},{
        "title": "Lecture-10 slides",
        "excerpt":"Dear students, Lecture 10 (Rethinking sensing and sampling) slides are now online. The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2019/01/06/lec10slides/",
        "teaser":null},{
        "title": "Guest lecture - slides",
        "excerpt":"Dear students, Slides of Prof. Michael Elad’s guest lecture on Regularization by Denoising are now available within Lecture-9 page. The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2019/01/07/guestlecslides/",
        "teaser":null},{
        "title": "Guidelines for presentations",
        "excerpt":"Dear students,       Presentations slides should be emailed to Sanketh no later than 20/01/2019.         We will soon publish the order of presentations. Everyone will present from the same computer to reduce overhead between the presenters.     The presentation should address briefly the following points:          Full project title and description      The problem setting      Paper’s contributions      Results      Suggested improvements, pros/cons etc.        Please keep your presentation short! Each group will be alloted 5 minutes of time.Best of luck! The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2019/01/12/guidelines4ppt/",
        "teaser":null},{
        "title": "Presentation schedule",
        "excerpt":"Dear students, Here is the link to the presentation schedule for the next Monday. We will follow the same order as mentioned in the above document. The whole seminar session is likely to exceed 2 hours. So, please email Sanketh in case you have a lesson right after and would like to reschedule your talk. Please also check the previous post regarding the guidelines for the presentation. Best of luck! The course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2019/01/14/presentationschedule/",
        "teaser":null},{
        "title": "Today's lecture and tutorial",
        "excerpt":"Dear students, Today’s lecture and tutorial material are online. All videos except today’s are up-to-date as well. Best of luck! The course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2019/01/14/tlect12tut12/",
        "teaser":null},{
        "title": "Last lecture & tutorial",
        "excerpt":"Dear students, Videos of the last lecture and tutorial are now online. Best of luck! The course staff. ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2019/01/16/lastlectut/",
        "teaser":null},{
        "title": "HW2 & Project report",
        "excerpt":"Dear students, Due to examinations, the submission deadline for the project reports and the homework 2 are postponed. You have to submit both of them by 28th February 11:55PM. Best of luck! The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2019/01/31/hw2/",
        "teaser":null},{
        "title": "HW2 & Project deadline extended",
        "excerpt":"Dear students, Due to popular requests, the submission deadline for the project report and the homework 2 are postponed. You have to submit both of them by 16th March 11:55PM. No more extensions are allowed beyond this date. Best of luck! The course staff ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/2019/02/27/announcemnt/",
        "teaser":null},{
        "title": "Linear Algebra Crash Course",
        "excerpt":"Introduction The purpose of this document is to quickly refresh (presumably) knownnotions in linear algebra. It contains a collection of facts related tovectors, matrices, and geometric entities they represent that we willuse heavily in our course. Even though this quick reminder may seemredundant or trivial to most of you (I hope), I still suggest at leastto skim through it, as it might present less common ways ofinterpretation of very familiar definitions and properties. And even ifyou discover nothing new in this document, it will at least be useful tointroduce notation. Notation In our course, we will deal almost exclusively with the field of realnumbers, which we denote by $\\RR$. An $n$-dimensional Euclidean spacewill be denoted as $\\RR^n$, and the space of $m \\times n$ matrices as$\\RR^{m \\times n}$. We will try to stick to a consistent typesettingdenoting a scalar $a$ with lowercase italic, a vector $\\bb{a}$ inlowercase bold, and a matrix $\\bb{A}$ in uppercase bold. Elements of avector or a matrix will be denoted using subindices as $a_i$ and$a_{ij}$, respectively. Unless stated otherwise, a vector is a columnvector, and we will write $\\bb{a} = (a_1, \\dots, a_n)^\\Tr$ to save space(here, $^\\Tr$ is the transpose of the row vector $(a_1,\\dots, a_n)$). Inthe same way, $\\bb{A} = (\\bb{a}_1,\\dots,\\bb{a}_n)$ will refer to amatrix constructed from $n$ column vectors $\\bb{a}_i$. We will denotethe zero vector by $\\bb{0}$, and the identity matrix by $\\bb{I}$. Linear and affine spaces Given a collection of vectors${ \\bb{v}_1,\\dots,\\bb{v}_m \\in \\bb{R}^n }$, a new vector$\\bb{b} = a_1 \\bb{v}_1 + \\dots + a_m \\bb{v}_m$, with$a_1,\\dots, a_m  \\in \\RR$ some scalars, is called a linear combinationof the $\\bb{v}_i$’s. The collection of all linear combinations is calleda linear subspace of $\\RR^n$, denoted by We will say that the $\\bb{v}_i$’s span the linear subspace $\\mathcal{L}$. A vector $\\bb{u}$ that cannot be described as a linear combination of$\\bb{v}_1,\\dots,\\bb{v}_m$ (i.e., $\\bb{u} \\notin \\mathcal{L}$) is said tobe linearly independent of the latter vectors. The collection ofvectors ${ \\bb{v}_1,\\dots,\\bb{v}_m }$ is linearly independent if no$\\bb{v}_i$ is a linear combination of the rest of the vectors. In such acase, a vector in $\\mathcal{L}$ can be unambiguously defined by the setof $m$ scalars $a_1,\\dots, a_m$; omitting any of these scalars will makethe description incomplete. Formally, we say that $m$ is the dimensionof the subspace, $\\dim \\,\\mathcal{L} = m$. Geometrically, an $m$-dimensional subspace is an $m$-dimensional planepassing through the origin (a one-dimensional plane is a line, atwo-dimensional plane is the regular plane, and so on). The latter istrue, since setting all the $a_i$’s to zero in the definition of$\\mathcal{L}$ yields the zero vector. Figure 1(left) visualizes this fact. An affine subspace can be defined as a linear subspace shifted by avector $\\bb{b} \\in \\RR^n$ away from the origin: Exercise. Show that any affine subspace can be defined as The latter linear combination with the scalars restricted to unit sum iscalled an affine combination.                 Figure 1: One-dimensional linear (left) and affine (right) subspaces of $\\RR^2$.  Vector inner product and norms Given two vectors $\\bb{u}$ and $\\bb{v}$ in $\\RR^n$, we define theirinner (a.k.a. scalar or dot) product as Though the notion of an inner product is more general than this, we willlimit our attention exclusively to this particular case (and its matrixequivalent defined in the sequel). This is sometimes called the standardor the Euclidean inner product. The inner product defines or induces avector norm (it is also convenient to write $|\\bb{u}|^2 = \\bb{u}^\\Tr \\bb{u}$),which is called the Euclidean or the induced norm. A more general notion of a norm can be introduced axiomatically. We willsay that a non-negative scalar function$| \\cdot | : \\RR^n \\rightarrow \\RR_+$ is a norm if it satisfies thefollowing axioms for any scalar $a \\in \\RR$ and any vectors$\\bb{u},\\bb{v} \\in \\RR^n$       $|a \\bb{u} | = | a | | \\bb{u} |$ (this property is calledabsolute homogeneity);         $| \\bb{u} + \\bb{v} | \\le | \\bb{u} | + | \\bb{v} |$ (thisproperty is called subadditivity, and since a norm induces ametric (distance function), the geometric name for it is triangleinequality);         $| \\bb{u} | = 0$ iff1 $\\bb{u} = \\bb{0}$.   In this course, we will almost exclusively restrict our attention to thefollowing family of norms called the $\\ell_p$ norms: For $1 \\le p&lt;\\infty$, the $\\ell_p$ norm of a vector $\\bb{u} \\in \\RR^n$is defined as Note the sub-index $_p$. The Euclidean norm corresponds to $p=2$ (hence,the alternative name, the $\\ell_2$ norm) and, whenever confusion mayarise, we will denote it by $| \\cdot |_2$. Another important particular case of the $\\ell_p$ family of norms is the$\\ell_1$ norm  As we will see,when used to quantify errors for example in a regression or modelfitting task, the $\\ell_1$ norm (representing mean absolute error) ismore robust (i.e., less sensitive to noisy data) than the Euclidean norm(representing mean squared error). The selection of $p=1$ is thesmallest among the $\\ell_p$ norms for which the norm is convex. Inthis course, we will dedicate significant attention to this importantnotion, as convexity will have profound impact on solvability ofoptimization problems. Yet another important particular case is the $\\ell_\\infty$ norm The latter can beobtained as the limit of $\\ell_p$. Exercise. Show that . Geometrically, a norm measures the length of a vector; a vector oflength $|\\bb{u}|=1$ is said to be a unit vector (with respect to2that norm). The collection ${ \\bb{u} : | \\bb{u} | = 1 }$ of all unitvectors is called the unit circle (see figure).Note that the unit circle is indeed a “circle” only for the Euclideannorm. Similarly, the collection$B_r = { \\bb{u} : | \\bb{u} | \\le r }$ of all vectors with length nobigger than $r$ is called the ball of radius $r$ (w.r.t. a givennorm). Again, norm balls are round only for the Euclidean norm. Fromfigure 2 we can deduce that for $p &lt; q$, the$\\ell_p$ unit circle is fully contained in the $\\ell_q$ unit circle.This means that the $\\ell_p$ norm is bigger than the $\\ell_q$ norm inthe sense that $| \\bb{u} |_q \\le | \\bb{b}|_p$. Exercise. Show that $| \\bb{u} |_q \\le | \\bb{b}|_p$ for every $p &lt; q$.                 Figure 2: Unit circles with respect to different $\\ell_p$ norms in $\\RR^2$  Continuing the geometric interpretation, it is worthwhile mentioningseveral relations between the inner product and the $\\ell_2$ norm itinduces. The inner product of two ($\\ell_2$-) unit vectors measures thecosine of the angle between them or, more generally,where $\\theta$ is the angle between $\\bb{u}$ and $\\bb{v}$. Two vectorssatisfying $\\langle \\bb{u}, \\bb{v} \\rangle = 0$ are said to beorthogonal (if the vectors are unit, they are also said to beorthonormal) – the algebraic way of saying “perpendicular”. The following result is doubtlessly the most important inequality inlinear algebra (and, perhaps, in mathematics in general): Theorem: Cauchy-Schwartz inequality.Let $| \\cdot |$ be the norm induced by an inner product$\\langle \\cdot, \\cdot \\rangle$ on $\\RR^n$, Then, for any$\\bb{u}, \\bb{v} \\in \\RR^n$, with equality holding iff $\\bb{u}$ and $\\bb{v}$ are linearly dependent. Matrices A linear map from the $n$-dimensional linear space $\\RR^n$ to the$m$-dimensional linear space $\\RR^m$ is a function mapping$\\bb{u} \\in \\RR^n$ $\\bb{v} \\in \\RR^m$ according to The latter can beexpressed compactly using the matrix-vector product notation$\\bb{v} = \\bb{A} \\bb{u}$, where $\\bb{A}$ is the matrix with the elements$a_{ij}$. In other words, a matrix $\\bb{A} \\in \\RR^{m \\times n}$ is acompact way of expressing a linear map between $\\RR^m$ and $\\RR^n$. Anmatrix is said to be square of $m=n$; such a matrix defines anoperator mapping $\\RR^n$ to itself. A symmetric matrix is a squarematrix $\\bb{A}$ such that $\\bb{A}^\\Tr = \\bb{A}$. Recollecting our notion of linear combinations and linear subspaces,observe that the vector $\\bb{v} = \\bb{A}\\bb{u}$ is the linearcombination of the columns $\\bb{a}_1,\\dots,\\bb{a}_n$ of $\\bb{A}$ withthe weights $u_1,\\dots,u_n$. The linear subspace$\\bb{A} \\RR^m = { \\bb{A} \\bb{u} : \\bb{u} \\in \\RR^m \\ }$ is called thecolumns space of $\\bb{A}$. The space is $n$-dimensional if the columnsare linearly independent; otherwise, if $k$ columns are linearlydependent, the space is $n-k$ dimensional. The latter dimension iscalled the column rank of the matrix. By transposing the matrix, therow rank can be defined in the same way. The following result isfundamental in linear algebra: Theorem. The column rank and the row rank of a matrix are always equal. Exercise. Prove the above theorem. Since the row and the column ranks of a matrix are equal, we will simplyrefer to both as the rank, denoting $\\rank\\, \\bb{A}$. A square$n \\times n$ matrix is full rank if its rank is $n$, and is rankdeficient otherwise. Full rank is a necessary condition for a squarematrix to possess an inverse (Reminder: the inverse of a matrix$\\bb{A}$ is such a matrix $\\bb{B}$ that$\\bb{A}\\bb{B} = \\bb{B}\\bb{A} = \\bb{I}$; when the inverse exists, thematrix is called invertible and its inverse is denoted by$\\bb{A}^{-1}$). In this course, we will often encounter the trace of a square matrix,which is defined as the sum of its diagonal entries, The following property of thetrace will be particularly useful: Let $\\bb{A} \\in \\RR^{m \\times n}$ and $\\bb{B} \\in \\RR^{n \\times m}$.Then $\\trace(\\bb{A}\\bb{B}) = \\trace(\\bb{B}\\bb{A})$. This is in sharp contrast to the result of the product itself, which isgenerally not commutative. In particular, the squared norm $| \\bb{u} |^2 = \\bb{u}^\\Tr \\bb{u}$ canbe written as $\\trace(\\bb{u}^\\Tr \\bb{u}) = \\trace(\\bb{u}\\bb{u}^\\Tr)$. Wewill see many cases where such an apparently weird writing is veryuseful. The above property can be generalized to the product of $k$matrices $\\bb{A}_1 \\dots \\bb{A}_k$ by saying that$\\trace(\\bb{A}_1 \\dots \\bb{A}_k)$ is invariant under a cyclicpermutation of the factors as long as their product is defined. Forexample,$\\trace(\\bb{A}\\bb{B}^\\Tr\\bb{C}) = \\trace(\\bb{C}\\bb{A}\\bb{B}^\\Tr) = \\trace(\\bb{B}^\\Tr\\bb{C}\\bb{A})$(again, as long as the matrix dimensions are such that the products aredefined). Matrix inner product and norms The notion of an inner product can be extended to matrices by thinkingof an $m \\times n$ matrix as of a long vector $m \\times n$ vectorcontaining the matrix elements for example, in the column-stack order.We will denote such a vector as$\\vec(\\bb{A}) = (a_{11},\\dots,a_{m1},a_{12},\\dots,a_{m2},\\dots,a_{1n},\\dots,a_{mn})^\\Tr$.With such an interpretation, we can define the inner product of twomatrices as Exercise. Show that $\\langle \\bb{A}, \\bb{B} \\rangle = \\trace( \\bb{A}^\\Tr \\bb{B} )$. The inner product induces the standard Euclidean norm on the space ofcolumn-stack representation of matrices, known as the Frobenius norm. Using the result of the exercise above,we can write Note the qualifier $_\\mathrm{F}$ used to distinguish this norm from other matrixnorm that we will define in the sequel. There exist another “standard” way of defining matrix norms by thinkingof an $m \\times n$ matrix $\\bb{A}$ as a linear operator mapping betweentwo normed spaces, say,$\\bb{A} : (\\RR^m,\\ell_p) \\rightarrow (\\RR^n,\\ell_q)$. Then, we candefine the operator norm measuring the maximum change of length (inthe $\\ell_q$ sense) of a unit (in the $\\ell_p$ sense) vector in theoperator domain: Exercise. Use the axioms of a norm to show that $| \\bb{A} |_{p,q}$ is a norm. Eigendecomposition Eigendecomposition (a.k.a. eigenvalue decomposition or in somecontexts spectral decomposition, from the German eigen for “self”)is doubtlessly the most important and useful forms of matrixfactorization. The following discussion will be valid only for squarematrices. Recall that an $n \\times n$ matrix $\\bb{A}$ represents a linear map on$\\RR^n$. In general, the effect of $\\bb{A}$ on a vector $\\bb{u}$ is anew vector $\\bb{v} = \\bb{A}\\bb{u}$, rotated and elongated or shrunk(and, potentially, reflected). However, there exist vectors which areonly elongated or shrunk by $\\bb{A}$. Such vectors are calledeigenvectors. Formally, an eigenvector of $\\bb{A}$ is a non-zerovector $\\bb{u}$ satisfying $\\bb{A}\\bb{u} = \\lambda \\bb{u}$, with thescalar $\\lambda$ (called eigenvalue) measuring the amount ofelongation or shrinkage of $\\bb{u}$ (if $\\lambda &lt; 0$, the vector isreflected). Note that the scale of an eigenvector has no meaning as itappears on both sides of the equation; for this reason, eigenvectors arealways normalized (in the $\\ell_2$ sense). For reasons not so relevantto our course, the collection of the eigenvalues is called thespectrum of a matrix. For an $n\\times n$ matrix $\\bb{A}$ with $n$ linearly independenteigenvectors we can write the following system Stacking the eigenvectors into the columns ofthe $n \\times n$ matrix $\\bb{U} = (\\bb{u}_1,\\dots,\\bb{u}_n)$, anddefining the diagonal matrix we can rewrite the system more compactly as$\\bb{A}\\bb{U} = \\bb{U}\\bb{\\Lambda}$. Independent eigenvectors means that$\\bb{U}$ is invertible, which leads to$\\bb{A} = \\bb{U}\\bb{\\Lambda}\\bb{U}^{-1}$. Geometrically,eigendecomposition of a matrix can be interpreted as a change ofcoordinates into a basis, in which the action of a matrix can bedescribed as elongation or shrinkage only (represented by the diagonalmatrix $\\bb{\\Lambda}$). If the matrix $\\bb{A}$ is symmetric, it can be shown that itseigenvectors are orthonormal, i.e.,$\\langle \\bb{u}_i, \\bb{u}_j \\rangle = \\bb{u}_i^\\Tr \\bb{u}_j = 0$ forevery $i \\ne j$ and, since the eigenvectors have unit length,$\\bb{u}_i^\\Tr \\bb{u} = 1$. This can be compactly written as$\\bb{U}^\\Tr \\bb{U} = \\bb{I}$ or, in other words,$\\bb{U}^{-1} = \\bb{U}^\\Tr$. Matrices satisfying this property are calledorthonormal or unitary, and we will say that symmetric matricesadmit unitary eigendecomposition$\\bb{A} = \\bb{U} \\bb{\\Lambda}\\bb{U}^\\Tr$. Exercise. Show that a symmetric matrix admits unitary eigendecomposition$\\bb{A} = \\bb{U} \\bb{\\Lambda}\\bb{U}^\\Tr$. Finally, we note two very simple but useful facts abouteigendecomposition:       $\\displaystyle{\\trace\\,\\bb{A} = \\trace(\\bb{U}\\bb{\\Lambda}\\bb{U}^{-1}) =  \\trace(\\bb{\\Lambda}\\bb{U}^{-1}\\bb{U}) = \\trace\\,\\bb{\\Lambda} = \\sum_{i=1}^n \\lambda_i}$.         $\\displaystyle{\\det\\bb{A} = \\det\\bb{U}\\det\\bb{\\Lambda}\\det\\bb{U}^{-1} =  \\det \\bb{U} \\det \\bb{\\Lambda} \\frac{1}{\\det \\bb{U}} = \\prod_{i=1}^n \\lambda_i}$.   In other words, the trace and the determinant of a matrix are given bythe sum and the product of its eigenvalues, respectively. Matrix functions Eigendecomposition is a very convenient way of performing various matrixoperations. For example, if we are given the eigendecomposition of$\\bb{A} = \\bb{U}\\bb{\\Lambda}\\bb{U}^{-1}$, its inverse can be expressedas$\\bb{A}^{-1} = (\\bb{U}\\bb{\\Lambda}\\bb{U}^{-1})^{-1} = \\bb{U}\\bb{\\Lambda}^{-1}\\bb{U}^{-1}$;however, since $\\bb{\\Lambda}$ is diagonal,$\\bb{\\Lambda}^{-1} =\\diag{1/\\lambda_1,\\dots,1/\\lambda_n}$. (This doesnot suggest, of course, that this is the computationally preferred wayto invert matrices, as the eigendecomposition itself is a costlyoperation). A similar idea can be applied to the square of a matrix:$\\bb{A}^2 = \\bb{U}\\bb{\\Lambda}\\bb{U}^{-1} \\bb{U}\\bb{\\Lambda}\\bb{U}^{-1} = \\bb{U}\\bb{\\Lambda}^2\\bb{U}^{-1}$and, again, we note that$\\bb{\\Lambda}^2 =\\diag{\\lambda_1^2,\\dots,\\lambda_n^2}$. By usinginduction, we can generalize this result to any integer power $p \\ge 0$:$\\bb{A}^p = \\bb{U}\\bb{\\Lambda}^p\\bb{U}^{-1}$. (Here, if, say, $p=1000$,the computational advantage of using eigendecomposition might be welljustified). Going one step further, let be a polynomial (either of a finite degree or an infinite series). We can applythis function to a square matrix $\\bb{A}$ as follows: Denoting by$\\varphi(\\bb{\\Lambda}) = \\diag{\\varphi(\\lambda_1),\\dots,\\varphi(\\lambda_n)}$the diagonal matrix formed by the element-wise application of the scalarfunction $\\varphi$ to the eigenvalues of $\\bb{A}$, we can writecompactly Finally, since many functions can be described polynomial series, we cangeneralize the latter definition to a (more or less) arbitrary scalar function$\\varphi$. The above procedure is a standard way of constructing a matrix function (thisterm is admittedly confusing, as we will see it assuming another meaning); forexample, matrix exponential and logarithm are constructed exactly like this.Note that the construction is sharply different from applying the function$\\varphi$ element-wise! Positive definite matrices Symmetric square matrices define an important family of functions calledquadratic forms that we will encounter very often in this course.Formally, a quadratic form is a scalar function on $\\RR^n$ given by where$\\bb{A}$ is a symmetric $n \\times n$ matrix, and $\\bb{x} \\in \\RR^n$. A symmetric square matrix $\\bb{A}$ is called positive definite(denoted as $\\bb{A} \\succ 0$) iff for every $\\bb{x} \\ne \\bb{0}$,$\\bb{x}^\\Tr \\bb{A} \\bb{x} &gt; 0$. The matrix is called positivesemi-definite (denoted as $\\bb{A} \\succeq 0$) if the inequality isweak. Positive (semi-) definite matrices can be equivalently defined throughtheir eigendecomposition: Let $\\bb{A}$ be a symmetric matrix admitting the eigendecomposition$\\bb{A} = \\bb{U}\\bb{\\Lambda}\\bb{U}^\\Tr$. Then $\\bb{A} \\succ 0$ iff$\\lambda_i &gt; 0$ for $i=1,\\dots,n$. Similarly, $\\bb{A} \\succeq 0$ iff$\\lambda_i \\ge 0$. In other words, the matrix is positive (semi-) definite if it haspositive (non-negative) spectrum. To get a hint why this is true,consider an arbitrary vector $\\bb{x} \\ne \\bb{0}$, and write Denoting $\\bb{y} = \\bb{U}^\\Tr \\bb{x}$, we have Since $\\bb{U}^\\Tr$ is full rank, the vector $\\bb{y}$ is also anarbitrary non-zero vector in $\\RR^n$ and the only way to make the lattersum always positive is by ensuring that all $\\lambda_i$ are positive.The very same reasoning is also true in the opposite direction. Geometrically, a quadratic form describes a second-order (hence the namequadratic) surface in $\\RR^n$, and the eigenvalues of the matrix$\\bb{A}$ can be interpreted as the surface curvature. Very informally,if a certain eigenvalue $\\lambda_i$ is positive, a small step in thedirection of the corresponding eigenvector $\\bb{u}_i$ rotates the normalto the surface in the same direction. The surface is said to havepositive curvature in that direction. Similarly, a negative eigenvaluecorresponds to the normal rotating in the opposite direction of the step(negative curvature). Finally, if $\\lambda_i = 0$, a step in thedirection of $\\bb{u}_i$ leave the normal unchanged (the surface is saidto be flat in that direction). A quadratic form created by a positivedefinite matrix represents a positively curved surface in alldirections. Such a surface is cup-shaped (if you can imagine an$n$-dimensional cup) or, formally, is convex; in the sequel, we willsee the important consequences this property has on optimizationproblems.             We will henceforth abbreviate “if and only if” as “iff”. Twostatements related by “iff” are equivalent; for example, if one ofthe statements is a definition of some object, and the other is itsproperty, the latter property can be used as an alternativedefinition. We will see many such examples. &#8617;               We will often abbreviate “with respect to” as “w.r.t.” &#8617;       ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/supplements/linear_algebra/",
        "teaser":null},{
        "title": "Multivariate Calculus",
        "excerpt":"Introduction The purpose of this document is to quickly refresh (presumably) knownnotions in multivariate differential calculus such as differentials,directional derivatives, the gradient and the Hessian. These notionswill be used heavily in our course. Even though this quick reminder mayseem redundant or trivial to most of you (I hope), I still suggest atleast to skim through it, as it might present less common ways ofinterpretation of very familiar definitions and properties. And even ifyou discover nothing new in this document, it will at least be useful tointroduce notation. Notation In our course, we will deal exclusively with real functions. A scalarfunction will be denoted as $f : \\RR^n \\rightarrow \\RR$, $f(\\bb{x})$, orsimply $f$. A vector-valued function will be denoted in bold, as$\\bb{f} : \\RR^n \\rightarrow \\RR^m$, or component-wise as$\\bb{f}(\\bb{x}) = (f_1(\\bb{x}), \\dots, f_m(\\bb{x}))^\\Tr$. A scalarfunction of a matrix variable, $f : \\RR^{m \\times n} \\rightarrow \\RR$,will be denoted as $f(\\bb{A})$, and a matrix-valued function of avector, $f : \\RR^n \\rightarrow \\RR^{m \\times k}$ as $\\bb{F}(\\bb{x})$.Derivatives of a scalar function of one variable will be denoted as$f’(x)$, $f’‘(x)$, etc. An $n$-times continuously differentiablefunction will be said $\\mathcal{C}^n$ ($f \\in \\mathcal{C}^n$). In mostcases, we will tacitly assume that a function is sufficiently smooth forat least the first-order derivative to exist. First-order derivative of a function of one variable Before proceeding to multivariate functions, let us remind ourselves a few basicnotions of univariate calculus. A $\\mathcal{C}^1$ function $f(x)$ can beapproximated linearly around some point $x=x_0$.  Incrementing the argument by$dx$, the function itself changes by the amount that we denote by$\\Delta f = f(x_0+dx) - f(x_0)$, while the linear approximation changes by theamount denoted by $df$. For a sufficiently small $dx$ (more formally, in thelimit $|dx| \\rightarrow 0$), it can be shown that $\\Delta f = df + o(dx)$1.This means that for an infinitesimally small increment $dx$, the linearapproximation of the function becomes exact. In this limit, $df$ is called thedifferential of $f$, and the slope of the linear approximation, is called thefirst-order derivative of $f$, denoted $\\displaystyle{\\frac{df}{dx} =f’(x_0)}$.  Another way to express this fact is through the first-order Taylorexpansion of $f$ around $x_0$: which essentially says that a linear function whose value at $x_0$ matches thatof $f(x_0)$, and whose slope matches that of $f$ (expressed by $f’(x_0)$)approximates $f$ around $x_0$ up to some second-order error. Gradient We can extend the previous discussion straightforwardly to the$n$-dimensional case. Let $f$ now be a $\\mathcal{C}^1$ function on$\\RR^n$. The surface the function creates in $\\RR^{n+1}$ can beapproximated by an $n$-dimensional tangent plane (the multidimensionalanalog of linear approximation). Fixing a point $\\bb{x}_0$ and making asmall step $\\dx = (dx_1,\\dots,dx_n)^\\Tr$ (note that now $\\dx$ is avector), it can be shown that the change in the value of the linearapproximation is given by where $\\frac{\\partial f}{\\partial x_i}$ denotes the partial derivativeof $f$ at $\\bb{x}_0$. The latter formula is usually known as the totaldifferential. Arranging the partial derivatives into a vector$\\displaystyle{\\bb{g} = \\left( \\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_n} \\right)^\\Tr }$,the total differential can be expressed as the inner product$df = \\langle \\bb{g}, \\dx \\rangle$. The object $\\bb{g}$ appearing in theinner product is called the gradient of $f$ at point $\\bb{x}_0$, andwill be denoted by $\\nabla f (\\bb{x}_0)$ (the symbol $\\nabla$,graphically a rotated capital Delta, is pronounced “nabla”, from thegrecized Hebrew “nevel” for “harp”; $\\nabla$ is sometimes called thedel operator). While we can simply define the gradient as the vectorof partial derivatives, we will see that the definition through theinner product can often be more useful. Directional derivative In this course, we will often encounter situations where we areinterested in the behavior of a function along a line (formally, we saythat $f(\\bb{x})$ is restricted to the one-dimensional linear subspace$\\mathcal{L} = { \\bb{x}_0 + \\alpha \\bb{r} : \\alpha \\in \\RR }$, where$\\bb{x}_0$ is some fixed point, and $\\bb{r}$ is a fixed direction). Letuse define a new function of a single variable $\\alpha$,$\\varphi(\\alpha) = f(\\bb{x}_0 + \\alpha \\bb{r})$. Note that we can findthe first-order derivative of $\\varphi$, arriving at the followingimportant notion: which is called the directional derivative of $f$ at $\\bb{x}_0$ in thedirection $\\bb{r}$. The same way a derivative measures the rate of change of a function, adirectional derivative measures the rate of change of a multivariatefunction when we make a small step in a particular direction. Denoting $\\bb{g} = \\nabla f(\\bb{x}_0)$ and using our definition of thegradient as the inner product, we can write Identifying in the latter quantity an inner product of $d\\alpha$ withthe scalar $\\bb{g}^\\Tr \\bb{r}$, we can say that $\\bb{g}^\\Tr \\bb{r}$ isthe gradient of $\\varphi(\\alpha)$ at $\\alpha=0$, which coincides withthe first-order derivative, $\\varphi’(0) = \\bb{g}^\\Tr \\bb{r}$, as$\\varphi$ is a function of a single variable. We can summarize thisresult as the following: Property. The directional derivative of $f$ at $\\bb{x}_ {0}$ in the direction$\\bb{r}$ is obtained by projecting the gradient at $\\bb{x}_ {0}$ onto thedirection $\\bb{r}$, $f’_ {\\bb{r}} = {\\bb{r}}^\\Tr \\nabla f(\\bb{x}_ {0})$. Hessian In the case of a function of a single variable, we saw that thedifferential of $f$ was given by $df = f’(x) dx$. However, thefirst-order derivative $f’(x)$ is also a function of $x$, and we canagain express its differential as $df’ = f’‘(x) dx$, where $f’‘(x)$denotes the second-order derivative. This notion can be extended to themultivariate case. Recall our definition of the gradient through theinner product,  Thinking of the gradient as of avector-valued function on $\\RR^n$,$\\bb{g}(\\bb{x}) = (g_1(\\bb{x}),\\dots,g_n(\\bb{x}))^\\Tr$, we can write with each $\\bb{h}_i$ being the gradient of the $i$-thcomponent of the gradient vector $\\bb{g}$, Denoting by $\\bb{H} = (\\bb{h}_ 1,\\dots,\\bb{h}_ n)$, we can write compactly$\\dg = \\bb{H}^\\Tr \\bb{dx}$. The $n\\times n$ matrix $\\bb{H}$ containingall the second-order partial derivatives of $f$ as its elements iscalled the Hessian of $f$ at point $\\bb{x}$, and is also denoted2as $\\nabla^2 f(\\bb{x})$. We tacitly assumed that $f$ is $\\mathcal{C}^2$in order for the second-order derivatives to exist. A nice property of$\\mathcal{C}^2$ functions is that partial derivation is commutative,meaning that the order of taking second-order partial derivatives can beinterchanged:$\\displaystyle{h_{ij} = \\frac{\\partial^2 f }{\\partial x_i \\partial x_j} = \\frac{\\partial^2 f}{\\partial x_j \\partial x_i} = h_{ji} }$.Algebraically, this implies that the Hessian matrix is symmetric, and wecan write Second-order directional derivative Recall that we have previously considered the restriction of amultivariate function $f$ to a line,$\\varphi(\\alpha) = f(\\bb{x}_ 0 + \\alpha \\bb{r})$. This gave rise to thefirst-order directional derivative $f_{\\bb{r}}(\\bb{x}_ 0) = \\varphi’(0)$.In a similar way, we define the second-order directional derivative at$\\bb{x}_ 0$ in the direction $\\bb{r}$ as Considering $f’_{\\bb{r}}(\\bb{x}) = \\bb{r}^\\Tr \\bb{g}(\\bb{x})$ as afunction of $\\bb{x}$, we can write its differential as from where In other words, in order to get the second-order directional derivative in thedirection $\\bb{r}$, one has to evaluate the quadratic form $\\bb{r}^\\Tr \\bb{H}\\bb{r}$. Derivatives of linear and quadratic functions Let $\\bb{y} = \\bb{A}\\bb{x}$ be a general linear operator defined by an$m \\times n$ matrix. Its differential is given straightforwardly by Using this result, we will do a small exercise deriving gradients and Hessiansof linear and quadratic functions. As we will see, it is often convenient tostart with evaluating the differential of a function. Our first example is a linear function of the form$f(\\bb{x}) = \\bb{b}^\\Tr \\bb{x}$, where $\\bb{b}$ is a constant vector.Note that this function is a particular case of the previous result(with $\\bb{A} = \\bb{b}^\\Tr$), and we can write $df = \\bb{b}^\\Tr \\dx$.Comparing this to the general definition of the gradient,$df = \\bb{g}^\\Tr(\\bb{x}) \\dx$, we deduce that the gradient of $f$ isgiven by $\\nabla f(\\bb{x}) = \\bb{b}$. Note that the gradient of a linearfunction is constant – this generalizes the case of a linear function ofone variable, $f(x)= bx$, which has a constant derivative $f’(x) = b$. Our second example is a quadratic function of the form$f(\\bb{x}) = \\bb{x}^\\Tr \\bb{A} \\bb{x}$, where $\\bb{A}$ is an$n \\times n$ matrix. We again compute the differential by definition, Note that in the limit $| \\dx | \\rightarrow 0$, the third term(quadratic in $|\\dx|$) goes to zero much faster than the first twoterms (linear in $\\dx$), and can be therefore neglected3, leading to Again, recognizing in the latter expression an inner product with $\\dx$,we conclude that $\\nabla f(\\bb{x}) = (\\bb{A}^\\Tr + \\bb{A})\\bb{x}$. For asymmetric $\\bb{A}$, the latter simplifies to$\\nabla f(\\bb{x}) = 2\\bb{A} \\bb{x}$. Note that the gradient of aquadratic function is a linear function; furthermore, the latterexpression generalizes the univariate quadratic function $f(x) = ax^2$,whose first-order derivative $f’(x) = 2ax$ is linear. Since the gradient $\\bb{g}(\\bb{x}) = (\\bb{A}^\\Tr +\\bb{A})\\bb{x}$ of thequadratic function is linear, its differential is immediately given by$\\dg = (\\bb{A}^\\Tr +\\bb{A})\\dx$, from where we conclude that the Hessianof $f$ is $\\bb{H}(\\bb{x}) = \\bb{A}^\\Tr +\\bb{A}$ (or $2\\bb{A}$ in thesymmetric case). Note that the Hessian of a quadratic function isconstant, which coincides with the univariate case $f’’ (x) = 2a$. In the sequel, we will see more complicated examples of gradients andHessians. For a comprehensive reference on derivatives of matrix andvector expressions, the Matrix Cookbook4 is highly advisable. Multivariate Taylor expansion We have seen the Taylor expansion of a function of one variable as a wayto obtain a linear approximation. This construction can be generalizedto the multivariate case, as we show here, limiting the expansion tosecond order. Theorem: Second-order Taylor expansion.Let $f$ be a $\\mathcal{C}^2$ function on $\\RR^n$, $\\bb{x}$ some point,and $\\bb{r}$ a sufficient small vector. Then, The theorem say that up to a third-order error term, the function can beapproximated around $\\bb{x}$ by a quadratic function$q(\\bb{r}) = f + \\bb{g}^\\Tr \\bb{r} + \\frac{1}{2} \\bb{r}^\\Tr \\bb{H} \\bb{r}$(note that the function is quadratic in $\\bb{r}$, as $\\bb{x}$ isconstant, and so are $f=f(\\bb{x})$, $\\bb{g}$, and $\\bb{H}$). Out of allpossible quadratic approximations of $f$, the approximation described by$q(\\bb{r}) \\approx f(\\bb{x} + \\bb{r})$ is such that its value, slope,and curvature at $\\bb{x}$ (equivalently, at $\\bb{r} = \\bb{0}$) matchthose of $f$. The latter geometric quantities are captured,respectively, by the values of the function, its gradient, and itsHessian; in order to match the value, slope, and curvature of $f$, $q$has to satisfy $q(\\bb{0}) = f(\\bb{x})$,$\\nabla q(\\bb{0}) = \\nabla f(\\bb{x})$, and$\\nabla^2 q(\\bb{0}) = \\nabla^2 f(\\bb{x})$ (note that the gradient andthe Hessian of $q$ are w.r.t $\\bb{r}$, whereas the derivatives of $f$are w.r.t. $\\bb{x}$). To see that the later equalities hold, we firstobserve that $q(\\bb{0}) = f(\\bb{x})$. Next, using the fact that$q(\\bb{r})$ is quadratic, its gradient and Hessian (w.r.t. $\\bb{r}$) aregiven by $\\nabla q(\\bb{r}) = \\bb{g} + \\bb{H} \\bb{r}$ and$\\nabla^2 q(\\bb{r}) = \\bb{H} \\bb{r}$. Substituting $\\bb{r} = \\bb{0}$yields $\\nabla q(\\bb{0}) = \\bb{g}$ and $\\nabla^2 q(\\bb{r}) = \\bb{H}$. Gradient of a function of a matrix The notion of gradient can be generalized to functions of matrices. Let$f : \\RR^{m \\times n} \\rightarrow \\RR$ be such function evaluated atsome $\\bb{X}$. We can think of an equivalent function on $\\RR^{mn}$evaluated at $\\bb{x} = \\vec(\\bb{X})$, for which the gradient is definedsimply as the $mn$-dimensional vector of all partial derivatives. We cantherefore think of the gradient of $f(\\bb{X})$ at $\\bb{X}$ as of the$m \\times n$ matrix Previously, we have seen that an “external” definition of the gradientthrough an inner product is often more useful. Such a definition is alsovalid for matrix arguments. Recall our definition of the standard innerproduct on the space of $m\\times n$ matrices asfor $\\bb{A},\\bb{B} \\in \\RR^{m \\times n}$. Using the total differentialformula yields where $\\dX$ is now an $m\\times n$ matrix. The matrix $\\bb{G}$ appearingin the above identity can be defined as the gradient of $f$. Gradient of a nonlinear function We finish this brief introduction by deriving the gradient of a morecomplicated function of the form where$\\bb{X} \\in \\RR^{m\\times n}$, $\\bb{a} \\in \\RR^n$,$\\bb{b},\\bb{c} \\in \\RR^m$, and $\\varphi$ is a $\\mathcal{C}^1$ functionapplied element-wise. We will encounter such functions during the coursewhen dealing with nonlinear regression and classification applications.In machine learning, functions of this form constitute buildingblocks of more complicated functions called artificial neural networks.As before, we proceed by computing differentials and using the chainrule. Denoting $\\bb{u} = \\bb{X}^\\Tr \\bb{a} + \\bb{b}$, we have Since $\\varphi$ is applied element-wise to$\\bb{u}$, the differential of $\\bb{\\varphi} = \\varphi(\\bb{u})$ is givenby Next, we consider the function $\\bb{u}(\\bb{X}) = \\bb{X}^\\Tr \\bb{a} + \\bb{b}$;since it is linear in $\\bb{X}$, its differential is given by $\\du = \\dX^\\Tr\\bb{a}$. Finally, we consider the function $f(\\bb{\\varphi}) = \\bb{c}^\\Tr\\bb{\\varphi}$, which is linear in $\\bb{\\varphi}$ and has the differential$df = \\bb{c}^\\Tr \\dphi$. Combining these results and using simple properties of the matrix traceyields In the latter expression, we recognize in the second argument of the innerproduct the gradient of $f$ w.r.t. $\\bb{X}$,             The little-$o$ notation means that there exists some function of$dx$, $o(dx)$, going faster to zero than $dx$ (i.e.,$\\displaystyle{\\frac{o(dx)}{dx}} \\rightarrow 0$), but the exact formof this function is unimportant. On the other hand, the big-$O$notation, as in $O(dx^2)$, stands for some function that grows withthe same rate as $dx^2$ (i.e.,$\\displaystyle{\\lim_{|dx|\\rightarrow 0} \\frac{dx^2}{O(dx^2)} &lt; \\infty }$). &#8617;               Some people find the following abuse of notation helpful: Thinkingof the gradient of $f$ as of a differential operator of the form“$\\displaystyle{\\nabla = \\left(                            \\begin{array}{c}                            \\frac{\\partial }{\\partial x_1}                             \\vdots                             \\frac{\\partial }{\\partial x_n}                             \\end{array}                        \\right)}$”applied to $f$, the Hessian can be expressed by applying theoperator “$\\displaystyle{\\nabla^2 = \\nabla \\nabla^\\Tr =\\left(    \\begin{array}{c}    \\frac{\\partial }{\\partial x_1}     \\vdots     \\frac{\\partial }{\\partial x_n}     \\end{array}\\right)\\left(\\textstyle{    \\frac{\\partial }{\\partial x_1}},\\dots, \\textstyle{\\frac{\\partial }{\\partial x_n}}\\right) =\\left(    \\begin{array}{ccc}        \\frac{\\partial^2 }{\\partial x_1 \\partial x_1} &amp; \\cdots &amp; \\frac{\\partial^2 }{\\partial x_1 \\partial x_n}         \\vdots &amp;  \\ddots &amp; \\vdots         \\frac{\\partial^2 }{\\partial x_n \\partial x_1} &amp; \\cdots &amp; \\frac{\\partial^2 }{\\partial x_n \\partial x_n}     \\end{array}\\right)}$”. &#8617;               This “explanation” can be written rigorously using limits. Anotherway of getting the same result is the well-known rule of“differential of a product”, $d(fg) = df\\, g + f \\, dg$, which canbe generalized to the multivariate case as follows: Let $h$ be ascalar function given as the inner product of two vector-valuedfunctions, $h(\\bb{x}) = \\bb{f}^\\Tr(\\bb{x}) \\bb{g}(\\bb{x})$. Then,$dh = \\df^\\Tr \\bb{g} + \\bb{f}^\\Tr \\dg$. &#8617;               http://www2.imm.dtu.dk/pubdb/views/edoc_download.php/3274/pdf/imm3274.pdf &#8617;       ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/supplements/multivariate_calculus/",
        "teaser":null},{
        "title": "Probability and statistics: a survival guide",
        "excerpt":"Random variables Probability measure We start with a few elementary (and simplified) definitions from thetheory of probability. Let us fix a sample space $\\Omega = [0,1]$. ABorel set on $\\Omega$ is a set that can be formed from open intervalsof the form $(a,b), 0 \\le a&lt;b \\le 1$, through the operations ofcountable union, countable intersection, and set difference. We willdenote the collection of all Borel sets in $\\Omega$ as $\\Sigma$. It ispretty straightforward to show that $\\Sigma$ contains the empty set, isclosed under complement, and is closed under countable union. Such a setis known as $\\sigma$-algebra and its elements (subsets of$\\mathbb{R}$) are referred to as events. A probability measure $P$ on $\\Sigma$ is a function$P : \\Sigma \\rightarrow [0,1]$ satisfying $P(\\emptyset) = 0$,$P(\\mathbb{R}) = 1$ and additivity for every countable collection${ E _n \\in \\Sigma }$, Random variables A random variable $\\mathpzc{X}$ is a measurable map$\\mathpzc{X} : \\Omega \\rightarrow \\mathbb{R}$, i.e., a function suchthat for every $a$,${ \\mathpzc{X} \\le a } = { \\alpha : \\mathpzc{X}(\\alpha) \\le a  } \\in \\Sigma$.The map $\\mathpzc{X}$ pushes forward the probability measure $P$; thepushforward measure $\\mathpzc{X} _\\ast P$ is given by where$\\mathpzc{X}^{-1}(A) = { \\alpha  : X(\\alpha) \\in A }$ is the preimageof $A \\subseteq \\mathbb{R}$. (In short, we can write$\\mathpzc{X} _\\ast P = P\\mathpzc{X}^{-1}$). This pushforward probabilitymeasure $\\mathpzc{X} _\\ast P$ is usually referred to as the probabilitydistribution (or the law) of $\\mathpzc{X}$. When the range of $\\mathpzc{X}$ is finite or countably infinite, therandom variable is called discrete and its distribution can bedescribed by the probability mass function (PMF): which is a shorthand for$P(  {\\alpha : \\mathpzc{X}(\\alpha) = x } )$. Otherwise, $\\mathpzc{X}$is called a continuous random variable. Any random variable can bedescribed by the cumulative distribution function (CDF) which is a shorthandfor$F _{\\mathpzc{X}}(x) = P(  {\\alpha : \\mathpzc{X}(\\alpha) \\le x } )$. If$X$ is absolutely continuous, the CDF can be described by the integral wherethe integrand $f _{\\mathpzc{X}}$ is known as the probability densityfunction (PDF)1. Uniform distribution and uniformization A random variable $\\mathpzc{U}$ is said to be uniformly distributed on$[0,1]$ (denoted as $\\mathpzc{U} \\sim \\mathcal{U}[0,1]$) if In other words, themap $\\mathpzc{U}$ pushes forward the standard Lebesgue measure on$[0,1]$, $\\mathpzc{U} _\\ast P = \\lambda$. The corresponding CDF is$F _\\mathpzc{U}(u) = \\max{ 0, \\min{ 1, u } }$. Let $\\mathpzc{X}$ besome other random variable characterized by the CDF $F _\\mathpzc{X}$. Wedefine $\\mathpzc{U} = F _\\mathpzc{X}(\\mathpzc{X})$. Let us pick anarbitrary $x \\in \\mathbb{R}$ and let $u = F _\\mathpzc{X}(x) \\in [0,1]$.From monotonicity of the CDF, it follows that $\\mathpzc{U} \\le u$ if andonly if $\\mathpzc{X} \\le x$. Hence,$F _\\mathpzc{U}(u) = P(\\mathpzc{U} \\le u) = P(\\mathpzc{X} \\le x) = F _\\mathpzc{X}(x) = u$.We conclude that by transforming a random variable with its own CDFuniformizes it on the interval $[0,1]$. Applying the relation in inverse direction, let$\\mathpzc{U} \\sim \\mathcal{U}[0,1]$ and let $F$ be a valid CDF. Then,the random variable $\\mathpzc{X} = F^{-1}(\\mathpzc{U})$ is distributedwith the CDF $F _\\mathpzc{U} = F$. Expectation The expected value (a.k.a. the expectation or mean) of a randomvariable $\\mathpzc{X}$ is given by where the integral is the Lebesgue integral w.r.t. the measure $P$;whenever a probability density function exists, the latter can bewritten as Note that due to the linearity of integration, the expectation operator$\\mathbb{E}$ is linear. Using the Lebesgue integral notation, we canwrite for $E \\in \\Sigma$ where is the indicator function of $E$, which is by itself arandom variable. This relates the expectation of the indicator of anevent to its probability. Moments For any measurable function $g : \\mathbb{R} \\rightarrow \\mathbb{R}$,$\\mathpzc{Z} = g(\\mathpzc{X})$ is also a random variable with theexpectation Such an expectation is called a moment of $\\mathpzc{X}$. Particularly,the $k$-th order moment is obtained by setting $g(x) = x^k$, The expected valueitself is the first-order moment of $\\mathpzc{X}$, which is oftendenoted simply as $\\mu _\\mathpzc{X} = \\mu _{1}(\\mathpzc{X})$. Thecentral $k$-th order moment is obtained by setting$g(x) = (x - \\mu _\\mathpzc{X})^k$, A particularly important central second-order moment is the variance Random vectors Joint and marginal distributions A vector $\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ ofrandom variables is called a random vector. Its probabilitydistribution is defined as before as the pushforward measure$P = \\mathpzcb{X} _\\ast \\lambda$ Its is customary to treat $\\mathpzcb{X}$as a collection of $n$ random variables and define their joint CDF as As before, whenever the following holds the integrand $f _{\\mathpzcb{X}}$ is called the joint PDF of$\\mathpzcb{X}$. The more rigorous definition as the Radon-Nikodymderivative staysunaltered, only that now $\\lambda$ is the $n$-dimensional Lebesguemeasure. Note that the joint CDF of the sub-vector$(\\mathpzc{X} _2, \\dots, \\mathpzc{X} _n)$ is given by Such a distribiution is called marginal w.r.t. $\\mathpzc{X} _1$ and theprocess of obtaining it by substituting $x _1 = \\infty$ into$F _{\\mathpzcb{X}}$ is called marginalization. The corresponding actionin terms of the PDF consists of integration over $x _1$, Statistical independence A set $\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n$ of random variables iscalled statistically independent if their joint CDF iscoordinate-separable, i.e., can be written as the following tensorproduct An alternative definion can be given in terms of the PDF (whenever itexists): We will see a few additional alternative definitions in the sequel. Let$\\mathpzc{X}$ and $\\mathpzc{Y}$ be statistically-independent randomvariables with a PDF and let $\\mathpzc{Z} = \\mathpzc{X}+\\mathpzc{Y}$.Then, where we changed the variable $x$ to $x’ = x+y$. Differentiating w.r.t.$z$ yields Since $\\mathpzc{X}$ and $\\mathpzc{Y}$ are statistically-independent, wecan substitute$f _{\\mathpzc{X}\\mathpzc{Y}} = f _{\\mathpzc{X}} \\otimes f _{\\mathpzc{Y}}$yielding This result is known as the convolution theorem. Limit theorems Given independent identically distributed (i.i.d.) variables$\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n$ with mean $\\mu$ and variance$\\sigma^2$, we define their sample average as Note that $\\mathpzc{S} _n$ is also a random variable with$\\mu _{\\mathpzc{S} _n} = \\mu$ and$\\displaystyle{\\sigma^2 _{\\mathpzc{S} _n} = \\frac{\\sigma^2}{n}}$. It isstraightforward to see that the variance decays to zero in the limit$n \\rightarrow \\infty$, meaning that $\\mathpzc{S} _n$ approaches adeterministic variable $\\mathpzc{S} = \\mu$. However, a much strongerresult exists: the (strong) law of large numbers states that in thelimit $n \\rightarrow \\infty$, the sample average converges inprobability to the expected value, i.e., This fact is often denoted as$\\mathpzc{S} _n \\mathop{\\rightarrow}^P \\mu$. Furthermore, defining thenormalized deviation from the limit$\\mathpzc{D} _n = \\sqrt{n}(\\mathpzc{S} _n - \\mu)$, the central limittheorem states that $\\mathpzc{D} _n$ converges in distribution to$\\mathcal{N}(0,\\sigma^2)$, that is, its CDF converges pointwise to thatof the normal distribution. This is often denoted as$\\mathpzc{D} _n \\mathop{\\rightarrow}^D \\mathcal{N}(0,\\sigma^2)$. A slightly more general result is known as the delta method instatistics: if $g :  \\mathbb{R} \\rightarrow \\mathbb{R}$ is a$\\mathcal{C}^1$ function with non-vanishing derivative, then by theTaylor theorem, where $\\nu$ lies between $\\mathpzc{S} _n$ and $\\mu$. Since by the law oflarge numbers $\\mathpzc{S} _n \\mathop{\\rightarrow}^P \\mu$, we also have$\\nu \\mathop{\\rightarrow}^P \\mu$; since $g’$ is continuous,$g’(\\nu) \\mathop{\\rightarrow}^P g’(\\mu)$. Rearranging the terms andmultiplying by $\\sqrt{n}$ yields from where (formally, by invoking the Slutsky theorem): Joint moments Given a measurable function$\\bb{g} : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$, a (joint) moment of arandom vector $\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ is the last term migh be undefined if the PDF does not exist.The mean of a random vector is simply$\\bb{\\mu} _\\mathpzcb{X}  = \\mathbb{E} \\mathpzcb{X}$. Of particularimportance are the second-order joint moments of pairs of randomvariables, andits central version The latter quantity is known as the covariance of $\\mathpzc{X}$ and$\\mathpzc{Y}$. Two random variables $\\mathpzc{X}$ and $\\mathpzc{Y}$ with$r _{\\mathpzc{X}\\mathpzc{Y}} = 0$ are called orthogonal2 The variables with $\\sigma^2 _{\\mathpzc{X}\\mathpzc{Y}} = 0$ are calleduncorrelated. Note that for a statistically independent pair$(\\mathpzc{X},\\mathpzc{Y})$, However, the converse is not true, i.e., lack of correlation does notgenerally imply statistical independence (with the notable exception ofnormal variables). If $\\mathpzc{X}$ and $\\mathpzc{Y}$ are uncorrelatedand furthermore one of them is zero-mean, then they are also orthogonal(and the other way around). In general, the correlation matrix of a random vector$\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ is given by its $(i,j)$-th element is$(\\bb{R} _{\\mathpzcb{X}}) _{ij} = \\mathbb{E} \\mathpzc{X} _i \\mathpzc{X} _j$.Similarly, the covariance matrix is defined as the central counterpartof the above moment, its $(i,j)$-th element is$(\\bb{C} _{\\mathpzcb{X}}) _{ij} =\\mathrm{Cov}( \\mathpzc{X} _i , \\mathpzc{X} _j)$.Given another random vector$\\mathpzcb{Y} = (\\mathpzc{Y} _1, \\dots, \\mathpzc{Y} _m)$, thecross-correlation and cross-covariance matrices are defined as$\\bb{R} _{\\mathpzcb{X}\\mathpzcb{Y}} = \\mathbb{E}  \\mathpzcb{X} \\mathpzcb{Y}^\\Tr$and$\\bb{C} _{\\mathpzcb{X}\\mathpzcb{Y}} = \\mathbb{E}  (\\mathpzcb{X} - \\bb{\\mu} _\\mathpzcb{X} ) (\\mathpzcb{Y} - \\bb{\\mu} _\\mathpzcb{Y} )^\\Tr$,respectively. Linear transformations Let $\\mathpzcb{X} = (\\mathpzc{X} _1, \\dots, \\mathpzc{X} _n)$ be an$n$-dimensional random vector, $\\bb{A}$ and $m \\times n$ deterministicmatrix, and $\\bb{b}$ and $m$-dimensional deterministic vector. We definea random vector $\\mathpzcb{Y} = \\bb{A} \\mathpzcb{X} + \\bb{b} $ as theaffine transformation of $\\mathpzcb{X}$. Using linearity of theexpectation operator, it is straightforward to show that Estimation Let $\\mathpzcb{X}$ be a latent $n$-dimensional random vector, and let$\\mathpzcb{Y}$ be a statistically related $m$-dimensional observation(measurement). For example $\\mathpzcb{Y}$ can be a linearly transformedversion of $\\mathpzcb{X}$ corrupted by additive random noise,$\\mathpzcb{Y} = \\bb{A}\\mathpzcb{X} + \\mathpzcb{N}$. We might attemptusing the information $\\mathpzcb{Y}$ contains about $\\mathpzcb{X}$ inorder to estimate $\\mathpzcb{X}$. For that purpose, let us construct adeterministic function $\\bb{h} : \\RR^m \\rightarrow \\RR^n$ that we aregoing to call an estimator. Supplying a realization$\\mathpzcb{Y} = \\bb{y}$ to this estimator will produce a deterministicvector $\\hat{\\bb{x}} = \\bb{h}(\\bb{y})$, which is referred to as theestimate of $\\mathpzcb{X}$ given the measurement $\\bb{y}$. With someabuse of notation, we will henceforth denote $\\bb{h}(\\bb{y})$ as$\\hat{\\bb{x}}(\\bb{y})$. Note that supplying the random observationvector $\\mathpzcb{Y}$ to $\\hat{\\bb{x}}$ produces the random vector$\\hat{\\mathpzcb{X}} = \\hat{\\bb{x}}(\\mathpzcb{Y})$; here thedeterministic function $\\hat{\\bb{x}}$ acts as a random variabletransformation. Ideally, $\\hat{\\mathpzcb{X}}$ and $\\hat{\\mathpzcb{X}}$ should coincide;however, unless the measurement is perfect, there will be a discrepancy$\\mathpzcb{E} = \\hat{\\mathpzcb{X}} - \\mathpzcb{X}$ which we will referto as the error vector. Maximum likelihood For the sake of simplicity of exposition, let us focus on a very commonestimation setting with a linear forward model and an additivestatisticaly independent noise, i.e., where $\\bb{A}$ isa deterministic $m \\times n$ matrix and $\\mathpzcb{N}$ is independent of$\\mathpzcb{X}$. In this case, we can assert that the distribution of themeasurement $\\mathpzcb{Y}$ given the latent signal $\\mathpzcb{X}$ issimply the distribution of $\\mathpzcb{N}$ at$\\mathpzcb{N} = \\mathpzcb{Y}- \\bb{A} \\mathpzcb{X}$, Assuming i.i.d. noise (i.e., that the $N _i$’s are distributedidentically and independently of each other), the latter simplifies to aproduct of one-dimensional measures. Note that this is essentially aparametric family of distributions – each choice of $\\bb{x}$ yields adistribution $P _{\\mathpzcb{Y} | \\mathpzcb{X} = \\bb{x}}$ of$\\mathpzcb{Y}$. For the time being, let us treat the notation$\\mathpzcb{Y} | \\mathpzcb{X}$ just as a funny way of writing. Given an estimate $\\hat{\\bb{x}}$ of the true realization $\\bb{x}$ of$\\mathpzcb{X}$, we can measure its “quality” by measuring some distance$D$ from $P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\hat{\\bb{x}}}$ to the truedistribution $P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\bb{x}}$ that created$\\mathpzcb{Y}$, and try to minimize it. Our estimator of $\\bb{x}$ cantherefore be written as Note that we treat the quantity to be estimated as a deterministicparameter rather than a stochastic quantity. A standard way of measuring distance3 between distributions is theso-called Kullback-Leibler (KL) divergence. To define it, let $P$ and$Q$ be two probability measures (such that $P$ is absolutely continuousw.r.t. $Q$). Then, the KL divergence from Q to P is defined as In other words, itis the expectation of the logarithmic differences between theprobabilities $P$ and $Q$ when the expectation is taken over $P$. Thedivergence can be thought of as an (asymmetric) distance between the twodistributions. Let us have a closer look at the minimization objective Note that the first term (that can be recognized as the entropy of$\\log P _{\\mathpzcb{Y} | \\mathpzcb{X}=\\bb{x}}$) does not depend on theminimization variable; hence, we have Let us now assume that $N$ realization ${ \\bb{y} _1, \\dots, \\bb{y} _N }$of $\\mathpzcb{Y}$ are observed. In this case, we can express the jointp.d.f of the observations as the product of$f _\\mathpzcb{N} (\\bb{y} _i - \\bb{A} \\bb{x})$ or, taking the negativelogarithm, This function is known as the negative log likelihood function. By thelaw of large numbers, when $N$ approaches infinity, Behold our minimization objective! To recapitulate, recall that we started with minimizing the discrepancybetween the latent parametric distribution that generated theobservation and that associated with our estimator. However, a closerlook at the objective revealed that it is the limit of the negative loglikelihood when the sample size goes to infinity. The minimization ofthe Kullback-Leibler divergence is equivalent to maximization of thelikelihood of the data coming from a specific parametric distribution, For this reason, the former estimator is called maximum likelihood(ML). Conditioning Before treating maximum a posteriori estimation, we need to brieflyintroduce the important notion of conditioning and conditionaldistributions. Recall our construction of a probability space comprisingthe triplet $\\Omega$ (the sample space), $\\Sigma$ (the Borel sigmaalgebra), and $P$ (the probability measure). Let $X$ be a randomvariable and $B \\subset \\Sigma$ a sub sigma-algebra of $\\Sigma$. We canthen define the conditional expectation of $\\mathpzc{X}$ given $B$ asa random variable $\\mathpzc{Z} = \\mathbb{E} \\mathpzc{X} | B$ satisfyingfor every $E \\in B$ (we are omitting some technical details such as, e.g., integrabilitythat $\\mathpzc{X}$ has to satisfy). Given another random variable $\\mathpzc{Y}$, we say that it generates asigma algebra $\\sigma(\\mathpzc{Y})$ as the set of pre-images of allBorel sets in $\\mathbb{R}$, We can then use the previous definition to define the conditionalexpectation of $\\mathpzc{X}$ given $\\mathpzc{Y}$ as Conditional distribution Recall that expectation applied to indicator functions can be used todefine probability measures. In fact, for every $E \\in \\Sigma$, we mayconstruct the random variable $\\ind _E$, leading to$P(E) = \\mathbb{E} \\ind _E$. We now repeat the same, this time replacing$\\mathbb{E} $ with $\\mathbb{E} \\cdot | \\mathpzc{Y}$. For every$E \\in \\Sigma$, is a random variable that can be thought of as a transformation of therandom variable $\\mathpzc{Y}$ by the function $\\varphi$. We denote thisfunction as $P(E |\\mathpzc{Y})$ and refer to it as the (regular)conditional probability of event $E$ given $\\mathpzc{Y}$. It is easyto show that for every measurable set $B \\subset \\mathbb{R}$, Substituting $E = { \\mathpzc{X} \\in B}$ yields the conditionaldistribution of random variable $X$ given $\\mathpzc{Y}$, It can be easily shown that $P _{\\mathpzc{X} | \\mathpzc{Y}}$ is a validprobability measure on $\\Sigma$ and for every pair of measurable sets$A$ and $B$, If density exists, $P _{\\mathpzc{X} | \\mathpzc{Y}}$ can be describedusing the conditional p.d.f. $f _{\\mathpzc{X} | \\mathpzc{Y}}$ and thelatter identity can be rewritten in the form This essentially means that$f _{\\mathpzc{XY} } (x, y) = f _{\\mathpzc{X} | \\mathpzc{Y}} (x | y)  f _\\mathpzc{Y}(y)$.Integrating w.r.t. $y$ yields the so-called total probability formula We can also immediately observe that if $\\mathpzc{X}$ and $\\mathpzc{Y}$are statistically independent, we have from where $f _{\\mathpzc{X} | \\mathpzc{Y}} = f _{\\mathpzc{X}}$. In thiscase, conditioning on $\\mathpzc{Y}$ does not change our knowledge of$\\mathpzc{X}$. Bayes’ theorem One of the most celebrate (and useful) results related to conditionaldistributions is the following theorem named after Thomas Bayes.Exchanging the roles of $\\mathpzc{X}$ and $\\mathpzc{Y}$, we have re-arranging the terms, we have in terms of probability measures, the equivalent form is Law of total expectation Note that treating the conditional density$f _{\\mathpzc{X}|\\mathpzc{Y}}(x|y)$ just as a funnily-decorated p.d.f.with the argument $x$, we can write the following expectation integral With (a very accepted) abuse of notation, we denote it as“$\\mathbb{E} (\\mathpzc{X}|\\mathpzc{Y}=y)$”. Note, however, that this isa very different object from $\\mathbb{E} \\, \\mathpzc{X}|\\mathpzc{Y}$ –while the former is a deterministic value, the latter is a randomvariable (a transformation of $\\mathpzc{Y}$). In order to construct$\\mathbb{E} \\, \\mathpzc{X}|\\mathpzc{Y}$ out of$\\mathbb{E} (\\mathpzc{X}|\\mathpzc{Y}=y)$, we define the map$\\varphi : y \\mapsto \\mathbb{E} (\\mathpzc{X}|\\mathpzc{Y}=y)$ and applyit to the random variable $\\mathpzc{Y}$, obtaining$\\mathbb{E} \\, \\mathpzc{X}|\\mathpzc{Y} = \\varphi(Y)$. Again, with aslight abuse of notation, we can write this as Let us now take a regular expectation of the transformed variable$\\varphi(Y)$, which can be viewed as a generalized moment of$\\mathpzc{Y}$, Rearranging the integrands and using$f _{\\mathpzc{XY} }  = f _{\\mathpzc{X} | \\mathpzc{Y}}   f _\\mathpzc{Y}$, weobtain Stated differently, Thisresult is known as the smoothing theorem or the law of totalexpectation and can be thought of as an integral version of the law oftotal probability. Maximum a posteriori Recall that in maximum likelihood estimation we treated $\\mathpzcb{X}$as a deterministic parameter and tried to maximize the conditionalprobability $P(\\mathpzcb{Y} | \\mathpzcb{X})$. Let us now think of$\\mathpzcb{X}$ as of a random vector and maximize its probability giventhe data, Invoking the Bayes theorem yields In the Bayesian jargon, $P _{\\mathpzcb{X}}$ is called the priorprobability, that is, our initial knowledge about $\\mathpzcb{X}$ beforeany observation thereof was obtained; $P _{\\mathpzcb{X} | \\mathpzcb{Y}}$is called the posterior probability having accounted for themeasurement $\\mathpzcb{Y}$. Note that the term$P _{\\mathpzcb{Y} | \\mathpzcb{X}}$ is our good old likelihood. Since weare maximizing the posterior probability, the former estimator is calledmaximum a posteriori (MAP). Taking negative logarithm, we obtain This yields the following expression for the MAP estimator The minimization objective looks very similar to what we had in the MLcase; the only difference is that now a prior term is added. In theabsence of a good prior, a uniform prior is typically assumed, whichreduces MAP estimation to ML estimation. Minimum mean squared error Another sound way of constructing the estimator function $\\hat{\\bb{x}}$is by minimizing some error criterion related to the error vector$\\mathcal{E}(\\mathpzcb{E})$. A very common pragmatic choice is the meansquared error (MSE) criterion, leading to the following optimization problem: The resulting estimator is called minimum mean squared error (or MMSE)estimator. Since the squared norm is coordinate separable, we caneffectively solve for each dimension of $\\hat{\\bb{x}}^{\\mathrm{MMSE}}$independently, finding the best (in the MSE sense) estimator of $X _i$given $\\mathpzcb{Y}$, The minimization objective can be written explicitly as The latter integral is minimized iff its non-negative integrand isminimized at every point $\\bb{y}$. Let us fix $\\bb{y}$ and define$a = h(\\bb{y})$. The expression to minimize is note that this is a convex quadratic function with the minimizer givenby Fromhere we conclude that consequently,the MMSE estimator of $\\mathpzcb{X}$ given $\\mathpzcb{Y}$ is given bythe conditional expectation The error vector produced by the MMSE estimator is given by$\\mathpzcb{E} =  \\mathbb{E} \\, \\mathpzcb{X} | \\mathpzcb{Y} - \\mathpzcb{X}$.Taking the expectation yields In other words, the estimation error is zero mean – a property oftenstated by saying that the MMSE estimator is unbiased. Since the MSE is equivalent (isomorphic) to Euclidean length, MMSEestimation can be viewed as the minimization of the length of the vector$\\mathpzcb{E}$ over the subspace of vectors of the form$\\hat{\\mathpzcb{X}} = \\bb{h}( \\mathpzcb{Y}  )$ with$\\bb{h} : \\RR^m \\rightarrow \\RR^m$. We known from Euclidean geometrythat the minimum length is obtained by the orthogonal projection of$\\mathpzcb{X}$ onto the said subspace, meaning that $\\hat{\\mathpzcb{X}}$is an MMSE estimator iff its error vector $\\mathpzcb{E}$ is orthogonalto every $\\bb{h}( \\mathpzcb{Y}  )$, that is, for every $\\bb{h} : \\RR^m \\rightarrow \\RR^m$. Best linear estimator Sometimes the functional dependence of$\\hat{\\mathpzcb{X}}^{\\mathrm{MMSE}} $ on $\\mathpzcb{Y}$ might be toocomplicated to compute. In that case, it is convenient to restrict thefamily of functions to some simple class such as that of linear (moreprecisely, affine) functions of the form$\\bb{h}(\\bb{y}) = \\bb{A} \\bb{y} + \\bb{b}$. The MMSE estimator restrictedto such a subspace of functions is known as the best linear estimator(BLE), and its optimal parameters $\\bb{A}$ and $\\bb{b}$ are found byminimizing Note that since$\\mathbb{E} \\mathpzcb{E} =   \\bb{A} \\mathbb{E}\\, \\mathpzcb{Y} + \\bb{b} - \\mathbb{E}\\, \\mathpzcb{X}$,we can always zero the estimator bias by setting$\\bb{b} = \\bb{\\mu} _{\\mathpzcb{X}} -  \\bb{A}\\bb{\\mu} _{\\mathpzcb{Y}}$.With this choice, the problem reduces to or, equivalently, Manipulating the order ofmultiplication under the trace, exchaging its order with that of theexpectation operator, and moving the constants outside the expectationyields the following minimization objective: Note that this is a convex (since$\\bb{C} _{\\mathpzcb{Y}}  \\succ 0$) quadratic function. In order to findits minimizer, we differentiate w.r.t. the parameter $\\bb{A}$ and equatethe gradient to zero: The optimal parameter is obtained as$\\bb{A} = \\bb{C} _{\\mathpzcb{X} \\mathpzcb{Y}}\\bb{C} _{\\mathpzcb{Y}}^{-1}$. Combining this result with the expression for $\\bb{b}$, the best linearestimator is As the moregeneral MMSE estimator, BLE is also unbiased and enjoys theorthogonality property, meaning that $\\hat{\\mathpzcb{X}}$ is an MMSEestimator iff its error vector $\\mathpzcb{E}$ is orthogonal to everyaffine function of $\\mathpzcb{Y}  )$, that is, for every $\\bb{A} \\in \\RR^{m \\times n}$ and $\\bb{b} \\in \\RR^m$.             To be completely rigorous, the proper way to define the PDF is byfirst equipping the image of the map $\\mathpzc{X}$ with the Lebesguemeasure $\\lambda$ that assigns to every interval $[a,b]$ its length$b-a$. Then, we invoke the Radon-Nikodym theorem saying that if$\\mathpzc{X}$ is absolutely continuous w.r.t. $\\lambda$, thereexists a measurable function $f : \\mathbb{R} \\rightarrow [0,\\infty)$such that for every measurable $A \\subset \\mathbb{R}$,$\\displaystyle{(\\mathpzc{X} _\\ast P)(A) =P(\\mathpzc{X}^{-1}(A)) = \\int _A f d\\lambda}$.$f$ is called the Radon-Nikodym derivative and denoted by$\\displaystyle{f = \\frac{d(\\mathpzc{X} _\\ast P)}{d\\lambda}} $. It isexactly our PDF. &#8617;               In fact, $r _{\\mathpzc{X}\\mathpzc{Y}}$ can be viewed as an innerproduct on the space of random variables. This creates a geometryisomorphic to the standard Euclidean metric in $\\mathbb{R}^n$. Usingthis construction, the Cauchy-Schwarz inequality immediatelyfollows:$| r _{\\mathpzc{X}\\mathpzc{Y}} | \\le \\sigma _\\mathpzc{X} \\sigma _\\mathpzc{Y}$. &#8617;               Actually, not a true metric (which what the term distanceimplies, but rather an asymmetric form thereof, formally termed adivergence. &#8617;       ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/supplements/probability_and_statistics/",
        "teaser":null},{
        "title": "Tutorial 1: Convolution in Python",
        "excerpt":"Material The notebook from the tutorial can be downloaded fromthe course tutorials repo. We recommend cloning the repo with git and using git pull to update it nexttime. Students are encouraged to install the provided conda environment and playwith the examples from the notebook. Video   Extra Resources       conda download andinstallation.         Detailed notebooks for learning pythonhttps://github.com/jerry-git/learn-python3         Good introduction to numpy (presentation + notebooks)https://github.com/gertingold/euroscipy-numpy-tutorial         Python official docshttps://docs.python.org/3.6/.         numpy official docshttps://docs.scipy.org/doc/numpy/reference/         scipy official docshttps://docs.scipy.org/   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/tutorials/tutorial_1/",
        "teaser":null},{
        "title": "Tutorial 2: Fourier transform",
        "excerpt":"Material The notebook from the tutorial can be downloaded fromthe course tutorials repo. Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/tutorials/tutorial_2/",
        "teaser":null},{
        "title": "Tutorial 3: Computed Tomography",
        "excerpt":"Material The notebook from the tutorial can be downloaded fromthe course tutorials repo. Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/tutorials/tutorial_3/",
        "teaser":null},{
        "title": "Tutorial 4: Sampling and Interpolation",
        "excerpt":"Material The notebook from the tutorial can be downloaded fromthe course tutorials repo. Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/tutorials/tutorial_4/",
        "teaser":null},{
        "title": "Tutorial 5: Inverse filtering",
        "excerpt":"Material The notebook from the tutorial can be downloaded fromthe course tutorials repo. Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/tutorials/tutorial_5/",
        "teaser":null},{
        "title": "Tutorial 6: ML vs MAP",
        "excerpt":"Material The notebook from the tutorial can be downloaded fromthe course tutorials repo. Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/tutorials/tutorial_6/",
        "teaser":null},{
        "title": "Tutorial 7: Bilateral filter, Non-local means",
        "excerpt":"Material The notebook from the tutorial can be downloaded fromthe course tutorials repo. Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/tutorials/tutorial_7/",
        "teaser":null},{
        "title": "Tutorial 8: PatchMatch",
        "excerpt":"Material The notebook from the tutorial can be downloaded fromthe course tutorials repo. Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/tutorials/tutorial_8/",
        "teaser":null},{
        "title": "Tutorial 9: L1-L2 optimization, BM3D",
        "excerpt":"Material The notebook from the tutorial can be downloaded fromthe course tutorials repo. Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/tutorials/tutorial_9/",
        "teaser":null},{
        "title": "Tutorial 10: Dictionary Learning, Deep Image Prior",
        "excerpt":"Material The notebook from the tutorial can be downloaded fromthe course tutorials repo. Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/tutorials/tutorial__10/",
        "teaser":null},{
        "title": "Tutorial 11: Deep learning for inverse problems and beyond",
        "excerpt":"Material The notebook from the tutorial can be downloaded fromthe course tutorials repo. Video   ","categories": [],
        "tags": [],
        "url": "https://vistalab-technion.github.io/cs236860/semesters/w1819/tutorials/tutorial__11/",
        "teaser":null}]
